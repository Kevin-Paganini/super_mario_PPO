13:4: not a valid test operator: (
13:4: not a valid test operator: 530.30.02
Defaulting to user installation because normal site-packages is not writeable
WARNING: Ignoring invalid distribution -ransformers (/home/paganinik/.local/lib/python3.10/site-packages)
WARNING: Ignoring invalid distribution -ransformers (/home/paganinik/.local/lib/python3.10/site-packages)
Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com
Requirement already satisfied: rllib in /home/paganinik/.local/lib/python3.10/site-packages (0.0.1)
Requirement already satisfied: ray in /home/paganinik/.local/lib/python3.10/site-packages (2.8.0)
Requirement already satisfied: gymnasium in /home/paganinik/.local/lib/python3.10/site-packages (0.29.1)
Requirement already satisfied: dm_tree in /home/paganinik/.local/lib/python3.10/site-packages (0.1.8)
Requirement already satisfied: pyboy in /home/paganinik/.local/lib/python3.10/site-packages (1.6.9)
Requirement already satisfied: pysdl2 in /home/paganinik/.local/lib/python3.10/site-packages (0.9.16)
Requirement already satisfied: pysdl2-dll in /home/paganinik/.local/lib/python3.10/site-packages (2.28.4)
Requirement already satisfied: tensorflow-probability in /home/paganinik/.local/lib/python3.10/site-packages (0.23.0)
Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from ray) (8.1.3)
Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray) (3.12.0)
Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray) (4.17.3)
Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray) (1.0.5)
Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from ray) (23.1)
Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.10/dist-packages (from ray) (3.20.3)
Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from ray) (6.0)
Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray) (1.3.1)
Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray) (1.3.3)
Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from ray) (2.29.0)
Requirement already satisfied: numpy>=1.19.3 in /home/paganinik/.local/lib/python3.10/site-packages (from ray) (1.26.0)
Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)
Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.5.0)
Requirement already satisfied: farama-notifications>=0.0.1 in /home/paganinik/.local/lib/python3.10/site-packages (from gymnasium) (0.0.4)
Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability) (1.4.0)
Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability) (1.16.0)
Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability) (5.1.1)
Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability) (0.4.0)
Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray) (23.1.0)
Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray) (0.19.3)
Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->ray) (3.1.0)
Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->ray) (3.4)
Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->ray) (1.26.15)
Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->ray) (2022.12.7)
WARNING: Ignoring invalid distribution -ransformers (/home/paganinik/.local/lib/python3.10/site-packages)
WARNING: Ignoring invalid distribution -ransformers (/home/paganinik/.local/lib/python3.10/site-packages)
WARNING: Ignoring invalid distribution -ransformers (/home/paganinik/.local/lib/python3.10/site-packages)
WARNING: Ignoring invalid distribution -ransformers (/home/paganinik/.local/lib/python3.10/site-packages)

[notice] A new release of pip is available: 23.1.2 -> 23.3.1
[notice] To update, run: python3 -m pip install --upgrade pip
2023-11-28 23:46:50,829	WARNING compression.py:16 -- lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.
2023-11-28 23:46:51,670	WARNING __init__.py:10 -- PG has/have been moved to `rllib_contrib` and will no longer be maintained by the RLlib team. You can still use it/them normally inside RLlib util Ray 2.8, but from Ray 2.9 on, all `rllib_contrib` algorithms will no longer be part of the core repo, and will therefore have to be installed separately with pinned dependencies for e.g. ray[rllib] and other packages! See https://github.com/ray-project/ray/tree/master/rllib_contrib#rllib-contrib for more information on the RLlib contrib effort.
UserWarning: Using SDL2 binaries from pysdl2-dll 2.28.4
2023-11-28 23:46:55,039	INFO worker.py:1673 -- Started a local Ray instance.
2023-11-28 23:47:18,645	WARNING __init__.py:10 -- A3C has/have been moved to `rllib_contrib` and will no longer be maintained by the RLlib team. You can still use it/them normally inside RLlib util Ray 2.8, but from Ray 2.9 on, all `rllib_contrib` algorithms will no longer be part of the core repo, and will therefore have to be installed separately with pinned dependencies for e.g. ray[rllib] and other packages! See https://github.com/ray-project/ray/tree/master/rllib_contrib#rllib-contrib for more information on the RLlib contrib effort.
2023-11-28 23:47:18,646	WARNING __init__.py:10 -- A2C has/have been moved to `rllib_contrib` and will no longer be maintained by the RLlib team. You can still use it/them normally inside RLlib util Ray 2.8, but from Ray 2.9 on, all `rllib_contrib` algorithms will no longer be part of the core repo, and will therefore have to be installed separately with pinned dependencies for e.g. ray[rllib] and other packages! See https://github.com/ray-project/ray/tree/master/rllib_contrib#rllib-contrib for more information on the RLlib contrib effort.
2023-11-28 23:47:18,660	WARNING __init__.py:10 -- AlphaZero has/have been moved to `rllib_contrib` and will no longer be maintained by the RLlib team. You can still use it/them normally inside RLlib util Ray 2.8, but from Ray 2.9 on, all `rllib_contrib` algorithms will no longer be part of the core repo, and will therefore have to be installed separately with pinned dependencies for e.g. ray[rllib] and other packages! See https://github.com/ray-project/ray/tree/master/rllib_contrib#rllib-contrib for more information on the RLlib contrib effort.
2023-11-28 23:47:18,994	WARNING __init__.py:10 -- ApexDQN has/have been moved to `rllib_contrib` and will no longer be maintained by the RLlib team. You can still use it/them normally inside RLlib util Ray 2.8, but from Ray 2.9 on, all `rllib_contrib` algorithms will no longer be part of the core repo, and will therefore have to be installed separately with pinned dependencies for e.g. ray[rllib] and other packages! See https://github.com/ray-project/ray/tree/master/rllib_contrib#rllib-contrib for more information on the RLlib contrib effort.
2023-11-28 23:47:19,009	WARNING __init__.py:10 -- DDPG has/have been moved to `rllib_contrib` and will no longer be maintained by the RLlib team. You can still use it/them normally inside RLlib util Ray 2.8, but from Ray 2.9 on, all `rllib_contrib` algorithms will no longer be part of the core repo, and will therefore have to be installed separately with pinned dependencies for e.g. ray[rllib] and other packages! See https://github.com/ray-project/ray/tree/master/rllib_contrib#rllib-contrib for more information on the RLlib contrib effort.
2023-11-28 23:47:19,009	WARNING __init__.py:10 -- ApexDDPG has/have been moved to `rllib_contrib` and will no longer be maintained by the RLlib team. You can still use it/them normally inside RLlib util Ray 2.8, but from Ray 2.9 on, all `rllib_contrib` algorithms will no longer be part of the core repo, and will therefore have to be installed separately with pinned dependencies for e.g. ray[rllib] and other packages! See https://github.com/ray-project/ray/tree/master/rllib_contrib#rllib-contrib for more information on the RLlib contrib effort.
2023-11-28 23:47:19,034	WARNING __init__.py:10 -- ES has/have been moved to `rllib_contrib` and will no longer be maintained by the RLlib team. You can still use it/them normally inside RLlib util Ray 2.8, but from Ray 2.9 on, all `rllib_contrib` algorithms will no longer be part of the core repo, and will therefore have to be installed separately with pinned dependencies for e.g. ray[rllib] and other packages! See https://github.com/ray-project/ray/tree/master/rllib_contrib#rllib-contrib for more information on the RLlib contrib effort.
2023-11-28 23:47:19,039	WARNING __init__.py:10 -- ARS has/have been moved to `rllib_contrib` and will no longer be maintained by the RLlib team. You can still use it/them normally inside RLlib util Ray 2.8, but from Ray 2.9 on, all `rllib_contrib` algorithms will no longer be part of the core repo, and will therefore have to be installed separately with pinned dependencies for e.g. ray[rllib] and other packages! See https://github.com/ray-project/ray/tree/master/rllib_contrib#rllib-contrib for more information on the RLlib contrib effort.
2023-11-28 23:47:19,056	WARNING __init__.py:10 -- BanditLinTS and BanditLinUCB has/have been moved to `rllib_contrib` and will no longer be maintained by the RLlib team. You can still use it/them normally inside RLlib util Ray 2.8, but from Ray 2.9 on, all `rllib_contrib` algorithms will no longer be part of the core repo, and will therefore have to be installed separately with pinned dependencies for e.g. ray[rllib] and other packages! See https://github.com/ray-project/ray/tree/master/rllib_contrib#rllib-contrib for more information on the RLlib contrib effort.
2023-11-28 23:47:19,121	WARNING __init__.py:10 -- CRR has/have been moved to `rllib_contrib` and will no longer be maintained by the RLlib team. You can still use it/them normally inside RLlib util Ray 2.8, but from Ray 2.9 on, all `rllib_contrib` algorithms will no longer be part of the core repo, and will therefore have to be installed separately with pinned dependencies for e.g. ray[rllib] and other packages! See https://github.com/ray-project/ray/tree/master/rllib_contrib#rllib-contrib for more information on the RLlib contrib effort.
2023-11-28 23:47:19,130	WARNING __init__.py:10 -- DDPPO has/have been moved to `rllib_contrib` and will no longer be maintained by the RLlib team. You can still use it/them normally inside RLlib util Ray 2.8, but from Ray 2.9 on, all `rllib_contrib` algorithms will no longer be part of the core repo, and will therefore have to be installed separately with pinned dependencies for e.g. ray[rllib] and other packages! See https://github.com/ray-project/ray/tree/master/rllib_contrib#rllib-contrib for more information on the RLlib contrib effort.
2023-11-28 23:47:19,142	WARNING __init__.py:8 -- `Dreamer(V1)` has been removed from RLlib and will no longer be maintained by the RLlib team. Use `DreamerV3` instead, which is part of the RLlib library and will continue to be maintained and supported in the future. See https://github.com/ray-project/ray/tree/master/rllib/algorithms/dreamerv3 for more information on our DreamerV3 implementation.
/home/paganinik/.local/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:135: UserWarning: [33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: float64[0m
  logger.warn(
/home/paganinik/.local/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:159: UserWarning: [33mWARN: The obs returned by the `reset()` method is not within the observation space.[0m
  logger.warn(f"{pre} is not within the observation space.")
2023-11-28 23:47:19,209	WARNING __init__.py:10 -- DT has/have been moved to `rllib_contrib` and will no longer be maintained by the RLlib team. You can still use it/them normally inside RLlib util Ray 2.8, but from Ray 2.9 on, all `rllib_contrib` algorithms will no longer be part of the core repo, and will therefore have to be installed separately with pinned dependencies for e.g. ray[rllib] and other packages! See https://github.com/ray-project/ray/tree/master/rllib_contrib#rllib-contrib for more information on the RLlib contrib effort.
2023-11-28 23:47:19,277	WARNING __init__.py:10 -- AlphaStar has/have been moved to `rllib_contrib` and will no longer be maintained by the RLlib team. You can still use it/them normally inside RLlib util Ray 2.8, but from Ray 2.9 on, all `rllib_contrib` algorithms will no longer be part of the core repo, and will therefore have to be installed separately with pinned dependencies for e.g. ray[rllib] and other packages! See https://github.com/ray-project/ray/tree/master/rllib_contrib#rllib-contrib for more information on the RLlib contrib effort.
2023-11-28 23:47:19,292	WARNING __init__.py:10 -- MADDDPG has/have been moved to `rllib_contrib` and will no longer be maintained by the RLlib team. You can still use it/them normally inside RLlib util Ray 2.8, but from Ray 2.9 on, all `rllib_contrib` algorithms will no longer be part of the core repo, and will therefore have to be installed separately with pinned dependencies for e.g. ray[rllib] and other packages! See https://github.com/ray-project/ray/tree/master/rllib_contrib#rllib-contrib for more information on the RLlib contrib effort.
2023-11-28 23:47:19,300	WARNING __init__.py:10 -- MAML has/have been moved to `rllib_contrib` and will no longer be maintained by the RLlib team. You can still use it/them normally inside RLlib util Ray 2.8, but from Ray 2.9 on, all `rllib_contrib` algorithms will no longer be part of the core repo, and will therefore have to be installed separately with pinned dependencies for e.g. ray[rllib] and other packages! See https://github.com/ray-project/ray/tree/master/rllib_contrib#rllib-contrib for more information on the RLlib contrib effort.
2023-11-28 23:47:19,313	WARNING __init__.py:10 -- MBMPO has/have been moved to `rllib_contrib` and will no longer be maintained by the RLlib team. You can still use it/them normally inside RLlib util Ray 2.8, but from Ray 2.9 on, all `rllib_contrib` algorithms will no longer be part of the core repo, and will therefore have to be installed separately with pinned dependencies for e.g. ray[rllib] and other packages! See https://github.com/ray-project/ray/tree/master/rllib_contrib#rllib-contrib for more information on the RLlib contrib effort.
2023-11-28 23:47:19,327	WARNING __init__.py:10 -- QMIX has/have been moved to `rllib_contrib` and will no longer be maintained by the RLlib team. You can still use it/them normally inside RLlib util Ray 2.8, but from Ray 2.9 on, all `rllib_contrib` algorithms will no longer be part of the core repo, and will therefore have to be installed separately with pinned dependencies for e.g. ray[rllib] and other packages! See https://github.com/ray-project/ray/tree/master/rllib_contrib#rllib-contrib for more information on the RLlib contrib effort.
2023-11-28 23:47:19,338	WARNING __init__.py:10 -- R2D2 has/have been moved to `rllib_contrib` and will no longer be maintained by the RLlib team. You can still use it/them normally inside RLlib util Ray 2.8, but from Ray 2.9 on, all `rllib_contrib` algorithms will no longer be part of the core repo, and will therefore have to be installed separately with pinned dependencies for e.g. ray[rllib] and other packages! See https://github.com/ray-project/ray/tree/master/rllib_contrib#rllib-contrib for more information on the RLlib contrib effort.
2023-11-28 23:47:19,361	WARNING __init__.py:10 -- SlateQ has/have been moved to `rllib_contrib` and will no longer be maintained by the RLlib team. You can still use it/them normally inside RLlib util Ray 2.8, but from Ray 2.9 on, all `rllib_contrib` algorithms will no longer be part of the core repo, and will therefore have to be installed separately with pinned dependencies for e.g. ray[rllib] and other packages! See https://github.com/ray-project/ray/tree/master/rllib_contrib#rllib-contrib for more information on the RLlib contrib effort.
2023-11-28 23:47:19,368	WARNING __init__.py:10 -- TD3 has/have been moved to `rllib_contrib` and will no longer be maintained by the RLlib team. You can still use it/them normally inside RLlib util Ray 2.8, but from Ray 2.9 on, all `rllib_contrib` algorithms will no longer be part of the core repo, and will therefore have to be installed separately with pinned dependencies for e.g. ray[rllib] and other packages! See https://github.com/ray-project/ray/tree/master/rllib_contrib#rllib-contrib for more information on the RLlib contrib effort.
2023-11-28 23:47:19,379	WARNING __init__.py:10 -- LeelaChessZero has/have been moved to `rllib_contrib` and will no longer be maintained by the RLlib team. You can still use it/them normally inside RLlib util Ray 2.8, but from Ray 2.9 on, all `rllib_contrib` algorithms will no longer be part of the core repo, and will therefore have to be installed separately with pinned dependencies for e.g. ray[rllib] and other packages! See https://github.com/ray-project/ray/tree/master/rllib_contrib#rllib-contrib for more information on the RLlib contrib effort.
/home/paganinik/.local/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py:484: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS="ignore::DeprecationWarning"
`UnifiedLogger` will be removed in Ray 2.7.
  return UnifiedLogger(config, logdir, loggers=None)
/home/paganinik/.local/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS="ignore::DeprecationWarning"
The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.
  self._loggers.append(cls(self.config, self.logdir, self.trial))
/home/paganinik/.local/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS="ignore::DeprecationWarning"
The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.
  self._loggers.append(cls(self.config, self.logdir, self.trial))
/home/paganinik/.local/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS="ignore::DeprecationWarning"
The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.
  self._loggers.append(cls(self.config, self.logdir, self.trial))
2023-11-28 23:47:19,409	INFO tensorboardx.py:48 -- pip install "ray[tune]" to see TensorBoard files.
2023-11-28 23:47:19,409	WARNING unified.py:56 -- Could not instantiate TBXLogger: No module named 'tensorboardX'.
[36m(pid=955169)[0m lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.
[36m(RolloutWorker pid=955169)[0m 2023-11-28 23:47:30,339	WARNING __init__.py:10 -- PG has/have been moved to `rllib_contrib` and will no longer be maintained by the RLlib team. You can still use it/them normally inside RLlib util Ray 2.8, but from Ray 2.9 on, all `rllib_contrib` algorithms will no longer be part of the core repo, and will therefore have to be installed separately with pinned dependencies for e.g. ray[rllib] and other packages! See https://github.com/ray-project/ray/tree/master/rllib_contrib#rllib-contrib for more information on the RLlib contrib effort.
[36m(RolloutWorker pid=955169)[0m 2023-11-28 23:47:31,071	WARNING env.py:162 -- Your env doesn't have a .spec.max_episode_steps attribute. Your horizon will default to infinity, and your environment will not be reset.
[36m(RolloutWorker pid=955169)[0m 2023-11-28 23:47:31,115	WARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.complex_input_net.ComplexInputNetwork` has been deprecated. This will raise an error in the future!
[36m(RolloutWorker pid=955169)[0m 2023-11-28 23:47:31,116	WARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!
2023-11-28 23:47:31,375	WARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.complex_input_net.ComplexInputNetwork` has been deprecated. This will raise an error in the future!
2023-11-28 23:47:31,375	WARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!
2023-11-28 23:47:31,404	INFO trainable.py:164 -- Trainable.setup took 11.995 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
2023-11-28 23:47:31,405	WARNING util.py:62 -- Install gputil for GPU system monitoring.
2023-11-28 23:47:31,458	INFO trainable.py:585 -- Restored on 10.199.0.12 from checkpoint: Checkpoint(filesystem=local, path=/home/paganinik/super_mario_PPO/trained/125)
2023-11-28 23:49:15,404	WARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.multi_gpu_train_one_step` has been deprecated. This will raise an error in the future!
2023-11-28 23:50:45,812	WARNING ppo.py:558 -- The mean reward returned from the environment is 24739.3701171875 but the vf_clip_param is set to 100. Consider increasing it for policy: default_policy to improve value function convergence.
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   valuation_workers', 'execution_plan', 'export_model', 'export_policy_checkpoint', 'export_policy_model', 'from_checkpoint', 'from_state', 'get_auto_filled_metrics', 'get_config', 'get_current_ip_pid', 'get_default_config', 'get_default_policy_class', 'get_policy', 'get_state', 'get_weights', 'import_model', 'import_policy_model_from_h5', 'is_actor', 'iteration', 'learner_group', 'load_checkpoint', 'local_replay_buffer', 'log_result', 'logdir', 'merge_algorithm_configs', 'remote_requests_in_flight', 'remove_policy', 'reset', 'reset_config', 'resource_help', 'restore', 'restore_workers', 'reward_estimators', 'save', 'save_checkpoint', 'set_weights', 'setup', 'step', 'stop', 'sync_num_retries', 'sync_sleep_time', 'train', 'train_buffered', 'train_exec_impl', 'training_iteration', 'training_step', 'trial_id', 'trial_name', 'trial_resources', 'validate_config', 'validate_env', 'workers']
The agent encountered 1 episodes this iteration...
Model saved at iteration 0, steps trained: 1024.0, sample results: {'episode_reward_max': 31771420.0, 'episode_reward_min': 31771420.0, 'episode_reward_mean': 31771420.0, 'episode_len_mean': 1177.0, 'episode_media': {}, 'episodes_this_iter': 1, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [31771420.0], 'episode_lengths': [1177]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.23806447193684854, 'mean_inference_ms': 1.042171021569261, 'mean_action_processing_ms': 0.09914546703052776, 'mean_env_wait_ms': 23.974154500051156, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.003981590270996094, 'StateBufferConnector_ms': 0.003337860107421875, 'ViewRequirementAgentConnector_ms': 0.09303092956542969}}
The agent encountered 1 episodes this iteration...
The agent encountered 6 episodes this iteration...
The agent encountered 4 episodes this iteration...
The agent encountered 3 episodes this iteration...
The agent encountered 7 episodes this iteration...
Model saved at iteration 5, steps trained: 1024.0, sample results: {'episode_reward_max': 116400010.0, 'episode_reward_min': 4503630.0, 'episode_reward_mean': 30188548.181818184, 'episode_len_mean': 1105.090909090909, 'episode_media': {}, 'episodes_this_iter': 7, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [31771420.0, 86005330.0, 102656990.0, 51298320.0, 12063700.0, 5745160.0, 11577020.0, 12664180.0, 25531110.0, 14655010.0, 34064050.0, 27766480.0, 11819900.0, 7582720.0, 116400010.0, 15888610.0, 41648240.0, 5291160.0, 12096050.0, 11815940.0, 4503630.0, 21303030.0], 'episode_lengths': [1177, 3721, 3532, 2096, 399, 219, 466, 499, 907, 463, 1357, 1170, 473, 273, 3627, 634, 1505, 182, 399, 392, 158, 663]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.240408793024052, 'mean_inference_ms': 1.0351459838301884, 'mean_action_processing_ms': 0.09740322235486122, 'mean_env_wait_ms': 23.936485520766322, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.004072622819380326, 'StateBufferConnector_ms': 0.0034104694019664416, 'ViewRequirementAgentConnector_ms': 0.09585293856534091}}
The agent encountered 2 episodes this iteration...
The agent encountered 2 episodes this iteration...
The agent encountered 5 episodes this iteration...
The agent encountered 4 episodes this iteration...
The agent encountered 3 episodes this iteration...
Model saved at iteration 10, steps trained: 1024.0, sample results: {'episode_reward_max': 116400010.0, 'episode_reward_min': 4503630.0, 'episode_reward_mean': 32784592.63157895, 'episode_len_mean': 1162.8947368421052, 'episode_media': {}, 'episodes_this_iter': 3, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [31771420.0, 86005330.0, 102656990.0, 51298320.0, 12063700.0, 5745160.0, 11577020.0, 12664180.0, 25531110.0, 14655010.0, 34064050.0, 27766480.0, 11819900.0, 7582720.0, 116400010.0, 15888610.0, 41648240.0, 5291160.0, 12096050.0, 11815940.0, 4503630.0, 21303030.0, 77065000.0, 16914740.0, 43037570.0, 6744240.0, 103640780.0, 23257320.0, 14366070.0, 18279100.0, 39081640.0, 62530300.0, 20005010.0, 23813620.0, 34110970.0, 13983660.0, 66911790.0, 17924650.0], 'episode_lengths': [1177, 3721, 3532, 2096, 399, 219, 466, 499, 907, 463, 1357, 1170, 473, 273, 3627, 634, 1505, 182, 399, 392, 158, 663, 3390, 662, 1604, 221, 3715, 665, 403, 626, 1229, 1712, 619, 759, 933, 455, 2382, 503]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.24210932938409033, 'mean_inference_ms': 1.0404062067852713, 'mean_action_processing_ms': 0.09749548318662668, 'mean_env_wait_ms': 23.96594955831775, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.004229420109799034, 'StateBufferConnector_ms': 0.003440129129510177, 'ViewRequirementAgentConnector_ms': 0.09644157008120888}}
The agent encountered 3 episodes this iteration...
The agent encountered 6 episodes this iteration...
The agent encountered 3 episodes this iteration...
The agent encountered 1 episodes this iteration...
The agent encountered 4 episodes this iteration...
Model saved at iteration 15, steps trained: 1024.0, sample results: {'episode_reward_max': 142661340.0, 'episode_reward_min': 4503630.0, 'episode_reward_mean': 34431631.27272727, 'episode_len_mean': 1180.6363636363637, 'episode_media': {}, 'episodes_this_iter': 4, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [31771420.0, 86005330.0, 102656990.0, 51298320.0, 12063700.0, 5745160.0, 11577020.0, 12664180.0, 25531110.0, 14655010.0, 34064050.0, 27766480.0, 11819900.0, 7582720.0, 116400010.0, 15888610.0, 41648240.0, 5291160.0, 12096050.0, 11815940.0, 4503630.0, 21303030.0, 77065000.0, 16914740.0, 43037570.0, 6744240.0, 103640780.0, 23257320.0, 14366070.0, 18279100.0, 39081640.0, 62530300.0, 20005010.0, 23813620.0, 34110970.0, 13983660.0, 66911790.0, 17924650.0, 30442530.0, 65465120.0, 33055100.0, 54172790.0, 14009740.0, 6645760.0, 51485450.0, 22754600.0, 14972130.0, 45179400.0, 19534490.0, 24245320.0, 142661340.0, 24208300.0, 44147750.0, 19340290.0, 35605090.0], 'episode_lengths': [1177, 3721, 3532, 2096, 399, 219, 466, 499, 907, 463, 1357, 1170, 473, 273, 3627, 634, 1505, 182, 399, 392, 158, 663, 3390, 662, 1604, 221, 3715, 665, 403, 626, 1229, 1712, 619, 759, 933, 455, 2382, 503, 890, 1764, 1015, 1850, 447, 205, 1640, 716, 486, 1518, 535, 850, 4525, 859, 1599, 637, 1209]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.242831309663576, 'mean_inference_ms': 1.041668360221507, 'mean_action_processing_ms': 0.09749260725822378, 'mean_env_wait_ms': 23.974551872031626, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.0043981725519353695, 'StateBufferConnector_ms': 0.0034510005604137073, 'ViewRequirementAgentConnector_ms': 0.09655952453613281}}
The agent encountered 3 episodes this iteration...
The agent encountered 5 episodes this iteration...
The agent encountered 4 episodes this iteration...
The agent encountered 5 episodes this iteration...
The agent encountered 1 episodes this iteration...
Model saved at iteration 20, steps trained: 1024.0, sample results: {'episode_reward_max': 142661340.0, 'episode_reward_min': 4503630.0, 'episode_reward_mean': 33909590.82191781, 'episode_len_mean': 1133.876712328767, 'episode_media': {}, 'episodes_this_iter': 1, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [31771420.0, 86005330.0, 102656990.0, 51298320.0, 12063700.0, 5745160.0, 11577020.0, 12664180.0, 25531110.0, 14655010.0, 34064050.0, 27766480.0, 11819900.0, 7582720.0, 116400010.0, 15888610.0, 41648240.0, 5291160.0, 12096050.0, 11815940.0, 4503630.0, 21303030.0, 77065000.0, 16914740.0, 43037570.0, 6744240.0, 103640780.0, 23257320.0, 14366070.0, 18279100.0, 39081640.0, 62530300.0, 20005010.0, 23813620.0, 34110970.0, 13983660.0, 66911790.0, 17924650.0, 30442530.0, 65465120.0, 33055100.0, 54172790.0, 14009740.0, 6645760.0, 51485450.0, 22754600.0, 14972130.0, 45179400.0, 19534490.0, 24245320.0, 142661340.0, 24208300.0, 44147750.0, 19340290.0, 35605090.0, 38464500.0, 19015670.0, 96773770.0, 30421480.0, 16271240.0, 20974850.0, 7955100.0, 48040160.0, 26447920.0, 37827890.0, 28546360.0, 35267210.0, 33731010.0, 13805650.0, 30801070.0, 33922730.0, 16789430.0, 46604370.0], 'episode_lengths': [1177, 3721, 3532, 2096, 399, 219, 466, 499, 907, 463, 1357, 1170, 473, 273, 3627, 634, 1505, 182, 399, 392, 158, 663, 3390, 662, 1604, 221, 3715, 665, 403, 626, 1229, 1712, 619, 759, 933, 455, 2382, 503, 890, 1764, 1015, 1850, 447, 205, 1640, 716, 486, 1518, 535, 850, 4525, 859, 1599, 637, 1209, 1333, 537, 2719, 954, 513, 562, 260, 1327, 719, 1388, 864, 1263, 1030, 439, 1016, 981, 467, 1466]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.2430700929561723, 'mean_inference_ms': 1.0411641631059148, 'mean_action_processing_ms': 0.09740481973460514, 'mean_env_wait_ms': 23.97528290227095, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.004385268851502301, 'StateBufferConnector_ms': 0.003442372361274615, 'ViewRequirementAgentConnector_ms': 0.09639622413948791}}
The agent encountered 6 episodes this iteration...
The agent encountered 2 episodes this iteration...
The agent encountered 2 episodes this iteration...
The agent encountered 4 episodes this iteration...
The agent encountered 2 episodes this iteration...
Model saved at iteration 25, steps trained: 1024.0, sample results: {'episode_reward_max': 142661340.0, 'episode_reward_min': 4503630.0, 'episode_reward_mean': 35589172.02247191, 'episode_len_mean': 1172.3033707865168, 'episode_media': {}, 'episodes_this_iter': 2, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [31771420.0, 86005330.0, 102656990.0, 51298320.0, 12063700.0, 5745160.0, 11577020.0, 12664180.0, 25531110.0, 14655010.0, 34064050.0, 27766480.0, 11819900.0, 7582720.0, 116400010.0, 15888610.0, 41648240.0, 5291160.0, 12096050.0, 11815940.0, 4503630.0, 21303030.0, 77065000.0, 16914740.0, 43037570.0, 6744240.0, 103640780.0, 23257320.0, 14366070.0, 18279100.0, 39081640.0, 62530300.0, 20005010.0, 23813620.0, 34110970.0, 13983660.0, 66911790.0, 17924650.0, 30442530.0, 65465120.0, 33055100.0, 54172790.0, 14009740.0, 6645760.0, 51485450.0, 22754600.0, 14972130.0, 45179400.0, 19534490.0, 24245320.0, 142661340.0, 24208300.0, 44147750.0, 19340290.0, 35605090.0, 38464500.0, 19015670.0, 96773770.0, 30421480.0, 16271240.0, 20974850.0, 7955100.0, 48040160.0, 26447920.0, 37827890.0, 28546360.0, 35267210.0, 33731010.0, 13805650.0, 30801070.0, 33922730.0, 16789430.0, 46604370.0, 121984290.0, 25147640.0, 50346970.0, 32806330.0, 11282630.0, 10471100.0, 30661120.0, 30555820.0, 98977760.0, 28477000.0, 115326900.0, 27658440.0, 20345460.0, 16687640.0, 56990770.0, 14316310.0], 'episode_lengths': [1177, 3721, 3532, 2096, 399, 219, 466, 499, 907, 463, 1357, 1170, 473, 273, 3627, 634, 1505, 182, 399, 392, 158, 663, 3390, 662, 1604, 221, 3715, 665, 403, 626, 1229, 1712, 619, 759, 933, 455, 2382, 503, 890, 1764, 1015, 1850, 447, 205, 1640, 716, 486, 1518, 535, 850, 4525, 859, 1599, 637, 1209, 1333, 537, 2719, 954, 513, 562, 260, 1327, 719, 1388, 864, 1263, 1030, 439, 1016, 981, 467, 1466, 3333, 692, 1394, 976, 339, 349, 1003, 871, 3629, 876, 3893, 963, 569, 546, 1723, 406]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.24328644357106974, 'mean_inference_ms': 1.0410215075038887, 'mean_action_processing_ms': 0.09735689479894966, 'mean_env_wait_ms': 23.977837153169546, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.004324216521188115, 'StateBufferConnector_ms': 0.0034546584225772473, 'ViewRequirementAgentConnector_ms': 0.09658658102657018}}
The agent encountered 2 episodes this iteration...
The agent encountered 2 episodes this iteration...
The agent encountered 2 episodes this iteration...
The agent encountered 5 episodes this iteration...
The agent encountered 6 episodes this iteration...
Model saved at iteration 30, steps trained: 1024.0, sample results: {'episode_reward_max': 148264090.0, 'episode_reward_min': 4503630.0, 'episode_reward_mean': 35884439.4, 'episode_len_mean': 1152.6, 'episode_media': {}, 'episodes_this_iter': 6, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [11577020.0, 12664180.0, 25531110.0, 14655010.0, 34064050.0, 27766480.0, 11819900.0, 7582720.0, 116400010.0, 15888610.0, 41648240.0, 5291160.0, 12096050.0, 11815940.0, 4503630.0, 21303030.0, 77065000.0, 16914740.0, 43037570.0, 6744240.0, 103640780.0, 23257320.0, 14366070.0, 18279100.0, 39081640.0, 62530300.0, 20005010.0, 23813620.0, 34110970.0, 13983660.0, 66911790.0, 17924650.0, 30442530.0, 65465120.0, 33055100.0, 54172790.0, 14009740.0, 6645760.0, 51485450.0, 22754600.0, 14972130.0, 45179400.0, 19534490.0, 24245320.0, 142661340.0, 24208300.0, 44147750.0, 19340290.0, 35605090.0, 38464500.0, 19015670.0, 96773770.0, 30421480.0, 16271240.0, 20974850.0, 7955100.0, 48040160.0, 26447920.0, 37827890.0, 28546360.0, 35267210.0, 33731010.0, 13805650.0, 30801070.0, 33922730.0, 16789430.0, 46604370.0, 121984290.0, 25147640.0, 50346970.0, 32806330.0, 11282630.0, 10471100.0, 30661120.0, 30555820.0, 98977760.0, 28477000.0, 115326900.0, 27658440.0, 20345460.0, 16687640.0, 56990770.0, 14316310.0, 102814940.0, 28893410.0, 148264090.0, 29179710.0, 31946870.0, 107736520.0, 40933100.0, 33227720.0, 16763660.0, 24328120.0, 17860450.0, 22645170.0, 16830660.0, 6691480.0, 25632970.0, 23595610.0, 33204070.0], 'episode_lengths': [466, 499, 907, 463, 1357, 1170, 473, 273, 3627, 634, 1505, 182, 399, 392, 158, 663, 3390, 662, 1604, 221, 3715, 665, 403, 626, 1229, 1712, 619, 759, 933, 455, 2382, 503, 890, 1764, 1015, 1850, 447, 205, 1640, 716, 486, 1518, 535, 850, 4525, 859, 1599, 637, 1209, 1333, 537, 2719, 954, 513, 562, 260, 1327, 719, 1388, 864, 1263, 1030, 439, 1016, 981, 467, 1466, 3333, 692, 1394, 976, 339, 349, 1003, 871, 3629, 876, 3893, 963, 569, 546, 1723, 406, 3468, 818, 4754, 910, 1044, 3083, 1139, 1019, 466, 799, 569, 669, 479, 214, 879, 729, 1030]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.2437649859203585, 'mean_inference_ms': 1.0415726933557037, 'mean_action_processing_ms': 0.09730139866026682, 'mean_env_wait_ms': 23.984238063684963, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.004476070404052734, 'StateBufferConnector_ms': 0.0034761428833007812, 'ViewRequirementAgentConnector_ms': 0.09703350067138672}}
The agent encountered 2 episodes this iteration...
The agent encountered 6 episodes this iteration...
The agent encountered 3 episodes this iteration...
The agent encountered 7 episodes this iteration...
The agent encountered 3 episodes this iteration...
Model saved at iteration 35, steps trained: 1024.0, sample results: {'episode_reward_max': 148264090.0, 'episode_reward_min': 6645760.0, 'episode_reward_mean': 36104362.6, 'episode_len_mean': 1128.75, 'episode_media': {}, 'episodes_this_iter': 3, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [23257320.0, 14366070.0, 18279100.0, 39081640.0, 62530300.0, 20005010.0, 23813620.0, 34110970.0, 13983660.0, 66911790.0, 17924650.0, 30442530.0, 65465120.0, 33055100.0, 54172790.0, 14009740.0, 6645760.0, 51485450.0, 22754600.0, 14972130.0, 45179400.0, 19534490.0, 24245320.0, 142661340.0, 24208300.0, 44147750.0, 19340290.0, 35605090.0, 38464500.0, 19015670.0, 96773770.0, 30421480.0, 16271240.0, 20974850.0, 7955100.0, 48040160.0, 26447920.0, 37827890.0, 28546360.0, 35267210.0, 33731010.0, 13805650.0, 30801070.0, 33922730.0, 16789430.0, 46604370.0, 121984290.0, 25147640.0, 50346970.0, 32806330.0, 11282630.0, 10471100.0, 30661120.0, 30555820.0, 98977760.0, 28477000.0, 115326900.0, 27658440.0, 20345460.0, 16687640.0, 56990770.0, 14316310.0, 102814940.0, 28893410.0, 148264090.0, 29179710.0, 31946870.0, 107736520.0, 40933100.0, 33227720.0, 16763660.0, 24328120.0, 17860450.0, 22645170.0, 16830660.0, 6691480.0, 25632970.0, 23595610.0, 33204070.0, 29949840.0, 23044400.0, 93585030.0, 12881480.0, 28094870.0, 21952130.0, 21986000.0, 20120050.0, 61071300.0, 13014140.0, 38684210.0, 35332170.0, 29869820.0, 10000890.0, 24991000.0, 10932390.0, 18446910.0, 27750310.0, 29543700.0, 48118460.0, 44632690.0], 'episode_lengths': [665, 403, 626, 1229, 1712, 619, 759, 933, 455, 2382, 503, 890, 1764, 1015, 1850, 447, 205, 1640, 716, 486, 1518, 535, 850, 4525, 859, 1599, 637, 1209, 1333, 537, 2719, 954, 513, 562, 260, 1327, 719, 1388, 864, 1263, 1030, 439, 1016, 981, 467, 1466, 3333, 692, 1394, 976, 339, 349, 1003, 871, 3629, 876, 3893, 963, 569, 546, 1723, 406, 3468, 818, 4754, 910, 1044, 3083, 1139, 1019, 466, 799, 569, 669, 479, 214, 879, 729, 1030, 856, 752, 3442, 406, 747, 610, 675, 680, 1987, 376, 1335, 1064, 1007, 309, 739, 311, 639, 848, 1035, 1297, 1260]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.2444174246886118, 'mean_inference_ms': 1.0425025179164897, 'mean_action_processing_ms': 0.09726037719808636, 'mean_env_wait_ms': 23.997026524733197, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.004637718200683594, 'StateBufferConnector_ms': 0.003518819808959961, 'ViewRequirementAgentConnector_ms': 0.09705090522766113}}
The agent encountered 2 episodes this iteration...
The agent encountered 1 episodes this iteration...
The agent encountered 1 episodes this iteration...
The agent encountered 2 episodes this iteration...
The agent encountered 3 episodes this iteration...
Model saved at iteration 40, steps trained: 1024.0, sample results: {'episode_reward_max': 151757820.0, 'episode_reward_min': 6645760.0, 'episode_reward_mean': 40340228.1, 'episode_len_mean': 1266.25, 'episode_media': {}, 'episodes_this_iter': 3, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [66911790.0, 17924650.0, 30442530.0, 65465120.0, 33055100.0, 54172790.0, 14009740.0, 6645760.0, 51485450.0, 22754600.0, 14972130.0, 45179400.0, 19534490.0, 24245320.0, 142661340.0, 24208300.0, 44147750.0, 19340290.0, 35605090.0, 38464500.0, 19015670.0, 96773770.0, 30421480.0, 16271240.0, 20974850.0, 7955100.0, 48040160.0, 26447920.0, 37827890.0, 28546360.0, 35267210.0, 33731010.0, 13805650.0, 30801070.0, 33922730.0, 16789430.0, 46604370.0, 121984290.0, 25147640.0, 50346970.0, 32806330.0, 11282630.0, 10471100.0, 30661120.0, 30555820.0, 98977760.0, 28477000.0, 115326900.0, 27658440.0, 20345460.0, 16687640.0, 56990770.0, 14316310.0, 102814940.0, 28893410.0, 148264090.0, 29179710.0, 31946870.0, 107736520.0, 40933100.0, 33227720.0, 16763660.0, 24328120.0, 17860450.0, 22645170.0, 16830660.0, 6691480.0, 25632970.0, 23595610.0, 33204070.0, 29949840.0, 23044400.0, 93585030.0, 12881480.0, 28094870.0, 21952130.0, 21986000.0, 20120050.0, 61071300.0, 13014140.0, 38684210.0, 35332170.0, 29869820.0, 10000890.0, 24991000.0, 10932390.0, 18446910.0, 27750310.0, 29543700.0, 48118460.0, 44632690.0, 38525860.0, 61270480.0, 151757820.0, 120399080.0, 26726970.0, 126243620.0, 16362860.0, 115623410.0, 16104140.0], 'episode_lengths': [2382, 503, 890, 1764, 1015, 1850, 447, 205, 1640, 716, 486, 1518, 535, 850, 4525, 859, 1599, 637, 1209, 1333, 537, 2719, 954, 513, 562, 260, 1327, 719, 1388, 864, 1263, 1030, 439, 1016, 981, 467, 1466, 3333, 692, 1394, 976, 339, 349, 1003, 871, 3629, 876, 3893, 963, 569, 546, 1723, 406, 3468, 818, 4754, 910, 1044, 3083, 1139, 1019, 466, 799, 569, 669, 479, 214, 879, 729, 1030, 856, 752, 3442, 406, 747, 610, 675, 680, 1987, 376, 1335, 1064, 1007, 309, 739, 311, 639, 848, 1035, 1297, 1260, 1214, 2090, 4906, 4131, 865, 3647, 462, 3380, 456]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.24435607000139342, 'mean_inference_ms': 1.0416847868629453, 'mean_action_processing_ms': 0.09720238797499763, 'mean_env_wait_ms': 23.99503635893166, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.0046274662017822266, 'StateBufferConnector_ms': 0.0035135746002197266, 'ViewRequirementAgentConnector_ms': 0.09703946113586426}}
The agent encountered 4 episodes this iteration...
The agent encountered 1 episodes this iteration...
The agent encountered 3 episodes this iteration...
The agent encountered 2 episodes this iteration...
The agent encountered 2 episodes this iteration...
Model saved at iteration 45, steps trained: 1024.0, sample results: {'episode_reward_max': 151757820.0, 'episode_reward_min': 6691480.0, 'episode_reward_mean': 42956483.9, 'episode_len_mean': 1331.42, 'episode_media': {}, 'episodes_this_iter': 2, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [19534490.0, 24245320.0, 142661340.0, 24208300.0, 44147750.0, 19340290.0, 35605090.0, 38464500.0, 19015670.0, 96773770.0, 30421480.0, 16271240.0, 20974850.0, 7955100.0, 48040160.0, 26447920.0, 37827890.0, 28546360.0, 35267210.0, 33731010.0, 13805650.0, 30801070.0, 33922730.0, 16789430.0, 46604370.0, 121984290.0, 25147640.0, 50346970.0, 32806330.0, 11282630.0, 10471100.0, 30661120.0, 30555820.0, 98977760.0, 28477000.0, 115326900.0, 27658440.0, 20345460.0, 16687640.0, 56990770.0, 14316310.0, 102814940.0, 28893410.0, 148264090.0, 29179710.0, 31946870.0, 107736520.0, 40933100.0, 33227720.0, 16763660.0, 24328120.0, 17860450.0, 22645170.0, 16830660.0, 6691480.0, 25632970.0, 23595610.0, 33204070.0, 29949840.0, 23044400.0, 93585030.0, 12881480.0, 28094870.0, 21952130.0, 21986000.0, 20120050.0, 61071300.0, 13014140.0, 38684210.0, 35332170.0, 29869820.0, 10000890.0, 24991000.0, 10932390.0, 18446910.0, 27750310.0, 29543700.0, 48118460.0, 44632690.0, 38525860.0, 61270480.0, 151757820.0, 120399080.0, 26726970.0, 126243620.0, 16362860.0, 115623410.0, 16104140.0, 39847270.0, 42927210.0, 37723430.0, 14898880.0, 139454290.0, 18633070.0, 51261500.0, 48456750.0, 75974470.0, 37630170.0, 146941740.0, 30895860.0], 'episode_lengths': [535, 850, 4525, 859, 1599, 637, 1209, 1333, 537, 2719, 954, 513, 562, 260, 1327, 719, 1388, 864, 1263, 1030, 439, 1016, 981, 467, 1466, 3333, 692, 1394, 976, 339, 349, 1003, 871, 3629, 876, 3893, 963, 569, 546, 1723, 406, 3468, 818, 4754, 910, 1044, 3083, 1139, 1019, 466, 799, 569, 669, 479, 214, 879, 729, 1030, 856, 752, 3442, 406, 747, 610, 675, 680, 1987, 376, 1335, 1064, 1007, 309, 739, 311, 639, 848, 1035, 1297, 1260, 1214, 2090, 4906, 4131, 865, 3647, 462, 3380, 456, 1102, 1274, 1197, 415, 3991, 522, 1512, 1265, 2225, 1109, 4487, 834]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.2442404865089929, 'mean_inference_ms': 1.0407142272353997, 'mean_action_processing_ms': 0.09711900846223136, 'mean_env_wait_ms': 23.993873838823518, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.004561185836791992, 'StateBufferConnector_ms': 0.003504037857055664, 'ViewRequirementAgentConnector_ms': 0.09682631492614746}}
The agent encountered 4 episodes this iteration...
The agent encountered 2 episodes this iteration...
The agent encountered 2 episodes this iteration...
The agent encountered 4 episodes this iteration...
The agent encountered 2 episodes this iteration...
Model saved at iteration 50, steps trained: 1024.0, sample results: {'episode_reward_max': 151757820.0, 'episode_reward_min': 6691480.0, 'episode_reward_mean': 44089153.5, 'episode_len_mean': 1363.89, 'episode_media': {}, 'episodes_this_iter': 2, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [48040160.0, 26447920.0, 37827890.0, 28546360.0, 35267210.0, 33731010.0, 13805650.0, 30801070.0, 33922730.0, 16789430.0, 46604370.0, 121984290.0, 25147640.0, 50346970.0, 32806330.0, 11282630.0, 10471100.0, 30661120.0, 30555820.0, 98977760.0, 28477000.0, 115326900.0, 27658440.0, 20345460.0, 16687640.0, 56990770.0, 14316310.0, 102814940.0, 28893410.0, 148264090.0, 29179710.0, 31946870.0, 107736520.0, 40933100.0, 33227720.0, 16763660.0, 24328120.0, 17860450.0, 22645170.0, 16830660.0, 6691480.0, 25632970.0, 23595610.0, 33204070.0, 29949840.0, 23044400.0, 93585030.0, 12881480.0, 28094870.0, 21952130.0, 21986000.0, 20120050.0, 61071300.0, 13014140.0, 38684210.0, 35332170.0, 29869820.0, 10000890.0, 24991000.0, 10932390.0, 18446910.0, 27750310.0, 29543700.0, 48118460.0, 44632690.0, 38525860.0, 61270480.0, 151757820.0, 120399080.0, 26726970.0, 126243620.0, 16362860.0, 115623410.0, 16104140.0, 39847270.0, 42927210.0, 37723430.0, 14898880.0, 139454290.0, 18633070.0, 51261500.0, 48456750.0, 75974470.0, 37630170.0, 146941740.0, 30895860.0, 30872980.0, 58251390.0, 17067980.0, 38573750.0, 21543830.0, 91524160.0, 26087090.0, 18590790.0, 133764820.0, 33619390.0, 22041380.0, 58842900.0, 33851090.0, 68254600.0], 'episode_lengths': [1327, 719, 1388, 864, 1263, 1030, 439, 1016, 981, 467, 1466, 3333, 692, 1394, 976, 339, 349, 1003, 871, 3629, 876, 3893, 963, 569, 546, 1723, 406, 3468, 818, 4754, 910, 1044, 3083, 1139, 1019, 466, 799, 569, 669, 479, 214, 879, 729, 1030, 856, 752, 3442, 406, 747, 610, 675, 680, 1987, 376, 1335, 1064, 1007, 309, 739, 311, 639, 848, 1035, 1297, 1260, 1214, 2090, 4906, 4131, 865, 3647, 462, 3380, 456, 1102, 1274, 1197, 415, 3991, 522, 1512, 1265, 2225, 1109, 4487, 834, 901, 1921, 507, 1235, 730, 3390, 747, 540, 3801, 909, 656, 1589, 972, 2441]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.2442807445234475, 'mean_inference_ms': 1.0407013920525665, 'mean_action_processing_ms': 0.09708758963452556, 'mean_env_wait_ms': 23.996825900276527, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.004689216613769531, 'StateBufferConnector_ms': 0.003514528274536133, 'ViewRequirementAgentConnector_ms': 0.09766173362731934}}
The agent encountered 3 episodes this iteration...
The agent encountered 7 episodes this iteration...
The agent encountered 2 episodes this iteration...
The agent encountered 4 episodes this iteration...
The agent encountered 1 episodes this iteration...
Model saved at iteration 55, steps trained: 1024.0, sample results: {'episode_reward_max': 151757820.0, 'episode_reward_min': 6691480.0, 'episode_reward_mean': 44040517.5, 'episode_len_mean': 1367.11, 'episode_media': {}, 'episodes_this_iter': 1, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [30661120.0, 30555820.0, 98977760.0, 28477000.0, 115326900.0, 27658440.0, 20345460.0, 16687640.0, 56990770.0, 14316310.0, 102814940.0, 28893410.0, 148264090.0, 29179710.0, 31946870.0, 107736520.0, 40933100.0, 33227720.0, 16763660.0, 24328120.0, 17860450.0, 22645170.0, 16830660.0, 6691480.0, 25632970.0, 23595610.0, 33204070.0, 29949840.0, 23044400.0, 93585030.0, 12881480.0, 28094870.0, 21952130.0, 21986000.0, 20120050.0, 61071300.0, 13014140.0, 38684210.0, 35332170.0, 29869820.0, 10000890.0, 24991000.0, 10932390.0, 18446910.0, 27750310.0, 29543700.0, 48118460.0, 44632690.0, 38525860.0, 61270480.0, 151757820.0, 120399080.0, 26726970.0, 126243620.0, 16362860.0, 115623410.0, 16104140.0, 39847270.0, 42927210.0, 37723430.0, 14898880.0, 139454290.0, 18633070.0, 51261500.0, 48456750.0, 75974470.0, 37630170.0, 146941740.0, 30895860.0, 30872980.0, 58251390.0, 17067980.0, 38573750.0, 21543830.0, 91524160.0, 26087090.0, 18590790.0, 133764820.0, 33619390.0, 22041380.0, 58842900.0, 33851090.0, 68254600.0, 57919450.0, 32074670.0, 44456140.0, 36038220.0, 32016370.0, 18252040.0, 23553630.0, 14020750.0, 10926860.0, 31222990.0, 38700190.0, 34421110.0, 98690250.0, 13487820.0, 27009470.0, 21716660.0, 64452540.0], 'episode_lengths': [1003, 871, 3629, 876, 3893, 963, 569, 546, 1723, 406, 3468, 818, 4754, 910, 1044, 3083, 1139, 1019, 466, 799, 569, 669, 479, 214, 879, 729, 1030, 856, 752, 3442, 406, 747, 610, 675, 680, 1987, 376, 1335, 1064, 1007, 309, 739, 311, 639, 848, 1035, 1297, 1260, 1214, 2090, 4906, 4131, 865, 3647, 462, 3380, 456, 1102, 1274, 1197, 415, 3991, 522, 1512, 1265, 2225, 1109, 4487, 834, 901, 1921, 507, 1235, 730, 3390, 747, 540, 3801, 909, 656, 1589, 972, 2441, 1836, 849, 1334, 1120, 873, 508, 691, 425, 306, 837, 1132, 1066, 3569, 379, 890, 702, 1848]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.2444232035415733, 'mean_inference_ms': 1.041659590110152, 'mean_action_processing_ms': 0.09708875026675436, 'mean_env_wait_ms': 24.00376888242788, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.004873991012573242, 'StateBufferConnector_ms': 0.003515005111694336, 'ViewRequirementAgentConnector_ms': 0.09740138053894043}}
The agent encountered 1 episodes this iteration...
The agent encountered 0 episodes this iteration...
The agent encountered 2 episodes this iteration...
The agent encountered 1 episodes this iteration...
The agent encountered 2 episodes this iteration...
Model saved at iteration 60, steps trained: 1024.0, sample results: {'episode_reward_max': 205192380.0, 'episode_reward_min': 6691480.0, 'episode_reward_mean': 47104670.9, 'episode_len_mean': 1456.67, 'episode_media': {}, 'episodes_this_iter': 2, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [20345460.0, 16687640.0, 56990770.0, 14316310.0, 102814940.0, 28893410.0, 148264090.0, 29179710.0, 31946870.0, 107736520.0, 40933100.0, 33227720.0, 16763660.0, 24328120.0, 17860450.0, 22645170.0, 16830660.0, 6691480.0, 25632970.0, 23595610.0, 33204070.0, 29949840.0, 23044400.0, 93585030.0, 12881480.0, 28094870.0, 21952130.0, 21986000.0, 20120050.0, 61071300.0, 13014140.0, 38684210.0, 35332170.0, 29869820.0, 10000890.0, 24991000.0, 10932390.0, 18446910.0, 27750310.0, 29543700.0, 48118460.0, 44632690.0, 38525860.0, 61270480.0, 151757820.0, 120399080.0, 26726970.0, 126243620.0, 16362860.0, 115623410.0, 16104140.0, 39847270.0, 42927210.0, 37723430.0, 14898880.0, 139454290.0, 18633070.0, 51261500.0, 48456750.0, 75974470.0, 37630170.0, 146941740.0, 30895860.0, 30872980.0, 58251390.0, 17067980.0, 38573750.0, 21543830.0, 91524160.0, 26087090.0, 18590790.0, 133764820.0, 33619390.0, 22041380.0, 58842900.0, 33851090.0, 68254600.0, 57919450.0, 32074670.0, 44456140.0, 36038220.0, 32016370.0, 18252040.0, 23553630.0, 14020750.0, 10926860.0, 31222990.0, 38700190.0, 34421110.0, 98690250.0, 13487820.0, 27009470.0, 21716660.0, 64452540.0, 205192380.0, 204013710.0, 11293570.0, 94577750.0, 102325970.0, 20669000.0], 'episode_lengths': [569, 546, 1723, 406, 3468, 818, 4754, 910, 1044, 3083, 1139, 1019, 466, 799, 569, 669, 479, 214, 879, 729, 1030, 856, 752, 3442, 406, 747, 610, 675, 680, 1987, 376, 1335, 1064, 1007, 309, 739, 311, 639, 848, 1035, 1297, 1260, 1214, 2090, 4906, 4131, 865, 3647, 462, 3380, 456, 1102, 1274, 1197, 415, 3991, 522, 1512, 1265, 2225, 1109, 4487, 834, 901, 1921, 507, 1235, 730, 3390, 747, 540, 3801, 909, 656, 1589, 972, 2441, 1836, 849, 1334, 1120, 873, 508, 691, 425, 306, 837, 1132, 1066, 3569, 379, 890, 702, 1848, 6496, 6497, 351, 3439, 2718, 690]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.2444363442960022, 'mean_inference_ms': 1.0419139100873016, 'mean_action_processing_ms': 0.09708448123581775, 'mean_env_wait_ms': 24.005714765228227, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.004871368408203125, 'StateBufferConnector_ms': 0.0035119056701660156, 'ViewRequirementAgentConnector_ms': 0.09745597839355469}}
The agent encountered 3 episodes this iteration...
The agent encountered 0 episodes this iteration...
The agent encountered 2 episodes this iteration...
The agent encountered 2 episodes this iteration...
The agent encountered 2 episodes this iteration...
Model saved at iteration 65, steps trained: 1024.0, sample results: {'episode_reward_max': 205192380.0, 'episode_reward_min': 6691480.0, 'episode_reward_mean': 50861818.7, 'episode_len_mean': 1549.72, 'episode_media': {}, 'episodes_this_iter': 2, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [107736520.0, 40933100.0, 33227720.0, 16763660.0, 24328120.0, 17860450.0, 22645170.0, 16830660.0, 6691480.0, 25632970.0, 23595610.0, 33204070.0, 29949840.0, 23044400.0, 93585030.0, 12881480.0, 28094870.0, 21952130.0, 21986000.0, 20120050.0, 61071300.0, 13014140.0, 38684210.0, 35332170.0, 29869820.0, 10000890.0, 24991000.0, 10932390.0, 18446910.0, 27750310.0, 29543700.0, 48118460.0, 44632690.0, 38525860.0, 61270480.0, 151757820.0, 120399080.0, 26726970.0, 126243620.0, 16362860.0, 115623410.0, 16104140.0, 39847270.0, 42927210.0, 37723430.0, 14898880.0, 139454290.0, 18633070.0, 51261500.0, 48456750.0, 75974470.0, 37630170.0, 146941740.0, 30895860.0, 30872980.0, 58251390.0, 17067980.0, 38573750.0, 21543830.0, 91524160.0, 26087090.0, 18590790.0, 133764820.0, 33619390.0, 22041380.0, 58842900.0, 33851090.0, 68254600.0, 57919450.0, 32074670.0, 44456140.0, 36038220.0, 32016370.0, 18252040.0, 23553630.0, 14020750.0, 10926860.0, 31222990.0, 38700190.0, 34421110.0, 98690250.0, 13487820.0, 27009470.0, 21716660.0, 64452540.0, 205192380.0, 204013710.0, 11293570.0, 94577750.0, 102325970.0, 20669000.0, 126299420.0, 31175140.0, 57970730.0, 185744920.0, 134294240.0, 46307890.0, 56865050.0, 163953400.0, 22543190.0], 'episode_lengths': [3083, 1139, 1019, 466, 799, 569, 669, 479, 214, 879, 729, 1030, 856, 752, 3442, 406, 747, 610, 675, 680, 1987, 376, 1335, 1064, 1007, 309, 739, 311, 639, 848, 1035, 1297, 1260, 1214, 2090, 4906, 4131, 865, 3647, 462, 3380, 456, 1102, 1274, 1197, 415, 3991, 522, 1512, 1265, 2225, 1109, 4487, 834, 901, 1921, 507, 1235, 730, 3390, 747, 540, 3801, 909, 656, 1589, 972, 2441, 1836, 849, 1334, 1120, 873, 508, 691, 425, 306, 837, 1132, 1066, 3569, 379, 890, 702, 1848, 6496, 6497, 351, 3439, 2718, 690, 3329, 853, 1576, 5724, 3685, 1360, 1666, 4722, 628]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.24445433318240112, 'mean_inference_ms': 1.042281631219824, 'mean_action_processing_ms': 0.09707698046435709, 'mean_env_wait_ms': 24.008562850450488, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.00491642951965332, 'StateBufferConnector_ms': 0.003505229949951172, 'ViewRequirementAgentConnector_ms': 0.09720087051391602}}
The agent encountered 2 episodes this iteration...
The agent encountered 1 episodes this iteration...
The agent encountered 1 episodes this iteration...
The agent encountered 1 episodes this iteration...
The agent encountered 1 episodes this iteration...
Model saved at iteration 70, steps trained: 1024.0, sample results: {'episode_reward_max': 205192380.0, 'episode_reward_min': 6691480.0, 'episode_reward_mean': 54172722.4, 'episode_len_mean': 1649.38, 'episode_media': {}, 'episodes_this_iter': 1, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [22645170.0, 16830660.0, 6691480.0, 25632970.0, 23595610.0, 33204070.0, 29949840.0, 23044400.0, 93585030.0, 12881480.0, 28094870.0, 21952130.0, 21986000.0, 20120050.0, 61071300.0, 13014140.0, 38684210.0, 35332170.0, 29869820.0, 10000890.0, 24991000.0, 10932390.0, 18446910.0, 27750310.0, 29543700.0, 48118460.0, 44632690.0, 38525860.0, 61270480.0, 151757820.0, 120399080.0, 26726970.0, 126243620.0, 16362860.0, 115623410.0, 16104140.0, 39847270.0, 42927210.0, 37723430.0, 14898880.0, 139454290.0, 18633070.0, 51261500.0, 48456750.0, 75974470.0, 37630170.0, 146941740.0, 30895860.0, 30872980.0, 58251390.0, 17067980.0, 38573750.0, 21543830.0, 91524160.0, 26087090.0, 18590790.0, 133764820.0, 33619390.0, 22041380.0, 58842900.0, 33851090.0, 68254600.0, 57919450.0, 32074670.0, 44456140.0, 36038220.0, 32016370.0, 18252040.0, 23553630.0, 14020750.0, 10926860.0, 31222990.0, 38700190.0, 34421110.0, 98690250.0, 13487820.0, 27009470.0, 21716660.0, 64452540.0, 205192380.0, 204013710.0, 11293570.0, 94577750.0, 102325970.0, 20669000.0, 126299420.0, 31175140.0, 57970730.0, 185744920.0, 134294240.0, 46307890.0, 56865050.0, 163953400.0, 22543190.0, 23567620.0, 36255420.0, 136838430.0, 155421110.0, 158968930.0, 60888430.0], 'episode_lengths': [669, 479, 214, 879, 729, 1030, 856, 752, 3442, 406, 747, 610, 675, 680, 1987, 376, 1335, 1064, 1007, 309, 739, 311, 639, 848, 1035, 1297, 1260, 1214, 2090, 4906, 4131, 865, 3647, 462, 3380, 456, 1102, 1274, 1197, 415, 3991, 522, 1512, 1265, 2225, 1109, 4487, 834, 901, 1921, 507, 1235, 730, 3390, 747, 540, 3801, 909, 656, 1589, 972, 2441, 1836, 849, 1334, 1120, 873, 508, 691, 425, 306, 837, 1132, 1066, 3569, 379, 890, 702, 1848, 6496, 6497, 351, 3439, 2718, 690, 3329, 853, 1576, 5724, 3685, 1360, 1666, 4722, 628, 782, 1068, 3887, 4505, 5169, 1630]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.2444268120874354, 'mean_inference_ms': 1.042340079145796, 'mean_action_processing_ms': 0.09706501288977852, 'mean_env_wait_ms': 24.009389507949514, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.004808902740478516, 'StateBufferConnector_ms': 0.0035071372985839844, 'ViewRequirementAgentConnector_ms': 0.09722709655761719}}
The agent encountered 2 episodes this iteration...
The agent encountered 1 episodes this iteration...
The agent encountered 1 episodes this iteration...
The agent encountered 0 episodes this iteration...
The agent encountered 2 episodes this iteration...
Model saved at iteration 75, steps trained: 1024.0, sample results: {'episode_reward_max': 205192380.0, 'episode_reward_min': 9095200.0, 'episode_reward_mean': 59766639.2, 'episode_len_mean': 1827.07, 'episode_media': {}, 'episodes_this_iter': 2, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [29949840.0, 23044400.0, 93585030.0, 12881480.0, 28094870.0, 21952130.0, 21986000.0, 20120050.0, 61071300.0, 13014140.0, 38684210.0, 35332170.0, 29869820.0, 10000890.0, 24991000.0, 10932390.0, 18446910.0, 27750310.0, 29543700.0, 48118460.0, 44632690.0, 38525860.0, 61270480.0, 151757820.0, 120399080.0, 26726970.0, 126243620.0, 16362860.0, 115623410.0, 16104140.0, 39847270.0, 42927210.0, 37723430.0, 14898880.0, 139454290.0, 18633070.0, 51261500.0, 48456750.0, 75974470.0, 37630170.0, 146941740.0, 30895860.0, 30872980.0, 58251390.0, 17067980.0, 38573750.0, 21543830.0, 91524160.0, 26087090.0, 18590790.0, 133764820.0, 33619390.0, 22041380.0, 58842900.0, 33851090.0, 68254600.0, 57919450.0, 32074670.0, 44456140.0, 36038220.0, 32016370.0, 18252040.0, 23553630.0, 14020750.0, 10926860.0, 31222990.0, 38700190.0, 34421110.0, 98690250.0, 13487820.0, 27009470.0, 21716660.0, 64452540.0, 205192380.0, 204013710.0, 11293570.0, 94577750.0, 102325970.0, 20669000.0, 126299420.0, 31175140.0, 57970730.0, 185744920.0, 134294240.0, 46307890.0, 56865050.0, 163953400.0, 22543190.0, 23567620.0, 36255420.0, 136838430.0, 155421110.0, 158968930.0, 60888430.0, 143607720.0, 9095200.0, 198512080.0, 83403960.0, 199083530.0, 54289150.0], 'episode_lengths': [856, 752, 3442, 406, 747, 610, 675, 680, 1987, 376, 1335, 1064, 1007, 309, 739, 311, 639, 848, 1035, 1297, 1260, 1214, 2090, 4906, 4131, 865, 3647, 462, 3380, 456, 1102, 1274, 1197, 415, 3991, 522, 1512, 1265, 2225, 1109, 4487, 834, 901, 1921, 507, 1235, 730, 3390, 747, 540, 3801, 909, 656, 1589, 972, 2441, 1836, 849, 1334, 1120, 873, 508, 691, 425, 306, 837, 1132, 1066, 3569, 379, 890, 702, 1848, 6496, 6497, 351, 3439, 2718, 690, 3329, 853, 1576, 5724, 3685, 1360, 1666, 4722, 628, 782, 1068, 3887, 4505, 5169, 1630, 4677, 282, 6214, 3094, 5928, 1574]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.24440316826376787, 'mean_inference_ms': 1.0424858647969666, 'mean_action_processing_ms': 0.09705690637738737, 'mean_env_wait_ms': 24.010844504900852, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.004811525344848633, 'StateBufferConnector_ms': 0.003512144088745117, 'ViewRequirementAgentConnector_ms': 0.0974428653717041}}
The agent encountered 2 episodes this iteration...
The agent encountered 1 episodes this iteration...
The agent encountered 2 episodes this iteration...
The agent encountered 1 episodes this iteration...
The agent encountered 2 episodes this iteration...
Model saved at iteration 80, steps trained: 1024.0, sample results: {'episode_reward_max': 205192380.0, 'episode_reward_min': 9095200.0, 'episode_reward_mean': 63953365.6, 'episode_len_mean': 1953.19, 'episode_media': {}, 'episodes_this_iter': 2, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [61071300.0, 13014140.0, 38684210.0, 35332170.0, 29869820.0, 10000890.0, 24991000.0, 10932390.0, 18446910.0, 27750310.0, 29543700.0, 48118460.0, 44632690.0, 38525860.0, 61270480.0, 151757820.0, 120399080.0, 26726970.0, 126243620.0, 16362860.0, 115623410.0, 16104140.0, 39847270.0, 42927210.0, 37723430.0, 14898880.0, 139454290.0, 18633070.0, 51261500.0, 48456750.0, 75974470.0, 37630170.0, 146941740.0, 30895860.0, 30872980.0, 58251390.0, 17067980.0, 38573750.0, 21543830.0, 91524160.0, 26087090.0, 18590790.0, 133764820.0, 33619390.0, 22041380.0, 58842900.0, 33851090.0, 68254600.0, 57919450.0, 32074670.0, 44456140.0, 36038220.0, 32016370.0, 18252040.0, 23553630.0, 14020750.0, 10926860.0, 31222990.0, 38700190.0, 34421110.0, 98690250.0, 13487820.0, 27009470.0, 21716660.0, 64452540.0, 205192380.0, 204013710.0, 11293570.0, 94577750.0, 102325970.0, 20669000.0, 126299420.0, 31175140.0, 57970730.0, 185744920.0, 134294240.0, 46307890.0, 56865050.0, 163953400.0, 22543190.0, 23567620.0, 36255420.0, 136838430.0, 155421110.0, 158968930.0, 60888430.0, 143607720.0, 9095200.0, 198512080.0, 83403960.0, 199083530.0, 54289150.0, 133445420.0, 39986550.0, 18204320.0, 202626240.0, 37419370.0, 21923690.0, 147378130.0, 69302720.0], 'episode_lengths': [1987, 376, 1335, 1064, 1007, 309, 739, 311, 639, 848, 1035, 1297, 1260, 1214, 2090, 4906, 4131, 865, 3647, 462, 3380, 456, 1102, 1274, 1197, 415, 3991, 522, 1512, 1265, 2225, 1109, 4487, 834, 901, 1921, 507, 1235, 730, 3390, 747, 540, 3801, 909, 656, 1589, 972, 2441, 1836, 849, 1334, 1120, 873, 508, 691, 425, 306, 837, 1132, 1066, 3569, 379, 890, 702, 1848, 6496, 6497, 351, 3439, 2718, 690, 3329, 853, 1576, 5724, 3685, 1360, 1666, 4722, 628, 782, 1068, 3887, 4505, 5169, 1630, 4677, 282, 6214, 3094, 5928, 1574, 4438, 1404, 612, 6314, 1183, 703, 4251, 1875]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.24435367119063955, 'mean_inference_ms': 1.0426427369767335, 'mean_action_processing_ms': 0.09704432577281884, 'mean_env_wait_ms': 24.01270930853504, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.0048656463623046875, 'StateBufferConnector_ms': 0.003511190414428711, 'ViewRequirementAgentConnector_ms': 0.09743857383728027}}
The agent encountered 1 episodes this iteration...
The agent encountered 2 episodes this iteration...
The agent encountered 2 episodes this iteration...
The agent encountered 2 episodes this iteration...
The agent encountered 4 episodes this iteration...
Model saved at iteration 85, steps trained: 1024.0, sample results: {'episode_reward_max': 205192380.0, 'episode_reward_min': 9095200.0, 'episode_reward_mean': 68421423.7, 'episode_len_mean': 2078.91, 'episode_media': {}, 'episodes_this_iter': 4, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [48118460.0, 44632690.0, 38525860.0, 61270480.0, 151757820.0, 120399080.0, 26726970.0, 126243620.0, 16362860.0, 115623410.0, 16104140.0, 39847270.0, 42927210.0, 37723430.0, 14898880.0, 139454290.0, 18633070.0, 51261500.0, 48456750.0, 75974470.0, 37630170.0, 146941740.0, 30895860.0, 30872980.0, 58251390.0, 17067980.0, 38573750.0, 21543830.0, 91524160.0, 26087090.0, 18590790.0, 133764820.0, 33619390.0, 22041380.0, 58842900.0, 33851090.0, 68254600.0, 57919450.0, 32074670.0, 44456140.0, 36038220.0, 32016370.0, 18252040.0, 23553630.0, 14020750.0, 10926860.0, 31222990.0, 38700190.0, 34421110.0, 98690250.0, 13487820.0, 27009470.0, 21716660.0, 64452540.0, 205192380.0, 204013710.0, 11293570.0, 94577750.0, 102325970.0, 20669000.0, 126299420.0, 31175140.0, 57970730.0, 185744920.0, 134294240.0, 46307890.0, 56865050.0, 163953400.0, 22543190.0, 23567620.0, 36255420.0, 136838430.0, 155421110.0, 158968930.0, 60888430.0, 143607720.0, 9095200.0, 198512080.0, 83403960.0, 199083530.0, 54289150.0, 133445420.0, 39986550.0, 18204320.0, 202626240.0, 37419370.0, 21923690.0, 147378130.0, 69302720.0, 131046070.0, 69611440.0, 118068340.0, 47855840.0, 48032500.0, 133794120.0, 38297310.0, 63225910.0, 34523960.0, 32767060.0, 29220100.0], 'episode_lengths': [1297, 1260, 1214, 2090, 4906, 4131, 865, 3647, 462, 3380, 456, 1102, 1274, 1197, 415, 3991, 522, 1512, 1265, 2225, 1109, 4487, 834, 901, 1921, 507, 1235, 730, 3390, 747, 540, 3801, 909, 656, 1589, 972, 2441, 1836, 849, 1334, 1120, 873, 508, 691, 425, 306, 837, 1132, 1066, 3569, 379, 890, 702, 1848, 6496, 6497, 351, 3439, 2718, 690, 3329, 853, 1576, 5724, 3685, 1360, 1666, 4722, 628, 782, 1068, 3887, 4505, 5169, 1630, 4677, 282, 6214, 3094, 5928, 1574, 4438, 1404, 612, 6314, 1183, 703, 4251, 1875, 3581, 2524, 3773, 1265, 1301, 3802, 1140, 1777, 1113, 988, 958]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.24426678316213724, 'mean_inference_ms': 1.042808183349235, 'mean_action_processing_ms': 0.09702411001918199, 'mean_env_wait_ms': 24.015062941780013, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.0048139095306396484, 'StateBufferConnector_ms': 0.0034873485565185547, 'ViewRequirementAgentConnector_ms': 0.09755730628967285}}
The agent encountered 2 episodes this iteration...
The agent encountered 2 episodes this iteration...
The agent encountered 2 episodes this iteration...
The agent encountered 2 episodes this iteration...
The agent encountered 2 episodes this iteration...
Model saved at iteration 90, steps trained: 1024.0, sample results: {'episode_reward_max': 205192380.0, 'episode_reward_min': 9095200.0, 'episode_reward_mean': 67659647.0, 'episode_len_mean': 2041.58, 'episode_media': {}, 'episodes_this_iter': 2, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [16104140.0, 39847270.0, 42927210.0, 37723430.0, 14898880.0, 139454290.0, 18633070.0, 51261500.0, 48456750.0, 75974470.0, 37630170.0, 146941740.0, 30895860.0, 30872980.0, 58251390.0, 17067980.0, 38573750.0, 21543830.0, 91524160.0, 26087090.0, 18590790.0, 133764820.0, 33619390.0, 22041380.0, 58842900.0, 33851090.0, 68254600.0, 57919450.0, 32074670.0, 44456140.0, 36038220.0, 32016370.0, 18252040.0, 23553630.0, 14020750.0, 10926860.0, 31222990.0, 38700190.0, 34421110.0, 98690250.0, 13487820.0, 27009470.0, 21716660.0, 64452540.0, 205192380.0, 204013710.0, 11293570.0, 94577750.0, 102325970.0, 20669000.0, 126299420.0, 31175140.0, 57970730.0, 185744920.0, 134294240.0, 46307890.0, 56865050.0, 163953400.0, 22543190.0, 23567620.0, 36255420.0, 136838430.0, 155421110.0, 158968930.0, 60888430.0, 143607720.0, 9095200.0, 198512080.0, 83403960.0, 199083530.0, 54289150.0, 133445420.0, 39986550.0, 18204320.0, 202626240.0, 37419370.0, 21923690.0, 147378130.0, 69302720.0, 131046070.0, 69611440.0, 118068340.0, 47855840.0, 48032500.0, 133794120.0, 38297310.0, 63225910.0, 34523960.0, 32767060.0, 29220100.0, 92026300.0, 36345150.0, 89176030.0, 45856070.0, 46632810.0, 52776170.0, 76140630.0, 57145780.0, 123274140.0, 54110500.0], 'episode_lengths': [456, 1102, 1274, 1197, 415, 3991, 522, 1512, 1265, 2225, 1109, 4487, 834, 901, 1921, 507, 1235, 730, 3390, 747, 540, 3801, 909, 656, 1589, 972, 2441, 1836, 849, 1334, 1120, 873, 508, 691, 425, 306, 837, 1132, 1066, 3569, 379, 890, 702, 1848, 6496, 6497, 351, 3439, 2718, 690, 3329, 853, 1576, 5724, 3685, 1360, 1666, 4722, 628, 782, 1068, 3887, 4505, 5169, 1630, 4677, 282, 6214, 3094, 5928, 1574, 4438, 1404, 612, 6314, 1183, 703, 4251, 1875, 3581, 2524, 3773, 1265, 1301, 3802, 1140, 1777, 1113, 988, 958, 2481, 1081, 2495, 1297, 1247, 1701, 2221, 1544, 3548, 1904]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.24424753342547678, 'mean_inference_ms': 1.0431814886519348, 'mean_action_processing_ms': 0.09701753456997164, 'mean_env_wait_ms': 24.018388587044765, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.004788398742675781, 'StateBufferConnector_ms': 0.0034966468811035156, 'ViewRequirementAgentConnector_ms': 0.09772062301635742}}
The agent encountered 2 episodes this iteration...
The agent encountered 2 episodes this iteration...
The agent encountered 2 episodes this iteration...
The agent encountered 0 episodes this iteration...
The agent encountered 2 episodes this iteration...
Model saved at iteration 95, steps trained: 1024.0, sample results: {'episode_reward_max': 205192380.0, 'episode_reward_min': 9095200.0, 'episode_reward_mean': 70958299.4, 'episode_len_mean': 2149.12, 'episode_media': {}, 'episodes_this_iter': 2, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [48456750.0, 75974470.0, 37630170.0, 146941740.0, 30895860.0, 30872980.0, 58251390.0, 17067980.0, 38573750.0, 21543830.0, 91524160.0, 26087090.0, 18590790.0, 133764820.0, 33619390.0, 22041380.0, 58842900.0, 33851090.0, 68254600.0, 57919450.0, 32074670.0, 44456140.0, 36038220.0, 32016370.0, 18252040.0, 23553630.0, 14020750.0, 10926860.0, 31222990.0, 38700190.0, 34421110.0, 98690250.0, 13487820.0, 27009470.0, 21716660.0, 64452540.0, 205192380.0, 204013710.0, 11293570.0, 94577750.0, 102325970.0, 20669000.0, 126299420.0, 31175140.0, 57970730.0, 185744920.0, 134294240.0, 46307890.0, 56865050.0, 163953400.0, 22543190.0, 23567620.0, 36255420.0, 136838430.0, 155421110.0, 158968930.0, 60888430.0, 143607720.0, 9095200.0, 198512080.0, 83403960.0, 199083530.0, 54289150.0, 133445420.0, 39986550.0, 18204320.0, 202626240.0, 37419370.0, 21923690.0, 147378130.0, 69302720.0, 131046070.0, 69611440.0, 118068340.0, 47855840.0, 48032500.0, 133794120.0, 38297310.0, 63225910.0, 34523960.0, 32767060.0, 29220100.0, 92026300.0, 36345150.0, 89176030.0, 45856070.0, 46632810.0, 52776170.0, 76140630.0, 57145780.0, 123274140.0, 54110500.0, 117150860.0, 47848260.0, 55058340.0, 65158270.0, 101761500.0, 27446690.0, 190871510.0, 85419600.0], 'episode_lengths': [1265, 2225, 1109, 4487, 834, 901, 1921, 507, 1235, 730, 3390, 747, 540, 3801, 909, 656, 1589, 972, 2441, 1836, 849, 1334, 1120, 873, 508, 691, 425, 306, 837, 1132, 1066, 3569, 379, 890, 702, 1848, 6496, 6497, 351, 3439, 2718, 690, 3329, 853, 1576, 5724, 3685, 1360, 1666, 4722, 628, 782, 1068, 3887, 4505, 5169, 1630, 4677, 282, 6214, 3094, 5928, 1574, 4438, 1404, 612, 6314, 1183, 703, 4251, 1875, 3581, 2524, 3773, 1265, 1301, 3802, 1140, 1777, 1113, 988, 958, 2481, 1081, 2495, 1297, 1247, 1701, 2221, 1544, 3548, 1904, 3453, 1307, 1778, 1781, 3200, 822, 6040, 2842]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.24424689899865407, 'mean_inference_ms': 1.0435208651194519, 'mean_action_processing_ms': 0.0970167363818645, 'mean_env_wait_ms': 24.02122207993204, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.004855632781982422, 'StateBufferConnector_ms': 0.0035004615783691406, 'ViewRequirementAgentConnector_ms': 0.09776425361633301}}
The agent encountered 4 episodes this iteration...
The agent encountered 0 episodes this iteration...
The agent encountered 2 episodes this iteration...
The agent encountered 1 episodes this iteration...
The agent encountered 2 episodes this iteration...
Model saved at iteration 100, steps trained: 1024.0, sample results: {'episode_reward_max': 205192380.0, 'episode_reward_min': 9095200.0, 'episode_reward_mean': 72798434.3, 'episode_len_mean': 2198.05, 'episode_media': {}, 'episodes_this_iter': 2, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [21543830.0, 91524160.0, 26087090.0, 18590790.0, 133764820.0, 33619390.0, 22041380.0, 58842900.0, 33851090.0, 68254600.0, 57919450.0, 32074670.0, 44456140.0, 36038220.0, 32016370.0, 18252040.0, 23553630.0, 14020750.0, 10926860.0, 31222990.0, 38700190.0, 34421110.0, 98690250.0, 13487820.0, 27009470.0, 21716660.0, 64452540.0, 205192380.0, 204013710.0, 11293570.0, 94577750.0, 102325970.0, 20669000.0, 126299420.0, 31175140.0, 57970730.0, 185744920.0, 134294240.0, 46307890.0, 56865050.0, 163953400.0, 22543190.0, 23567620.0, 36255420.0, 136838430.0, 155421110.0, 158968930.0, 60888430.0, 143607720.0, 9095200.0, 198512080.0, 83403960.0, 199083530.0, 54289150.0, 133445420.0, 39986550.0, 18204320.0, 202626240.0, 37419370.0, 21923690.0, 147378130.0, 69302720.0, 131046070.0, 69611440.0, 118068340.0, 47855840.0, 48032500.0, 133794120.0, 38297310.0, 63225910.0, 34523960.0, 32767060.0, 29220100.0, 92026300.0, 36345150.0, 89176030.0, 45856070.0, 46632810.0, 52776170.0, 76140630.0, 57145780.0, 123274140.0, 54110500.0, 117150860.0, 47848260.0, 55058340.0, 65158270.0, 101761500.0, 27446690.0, 190871510.0, 85419600.0, 36377520.0, 20833190.0, 43643920.0, 52105410.0, 147578760.0, 94435840.0, 157970760.0, 70628800.0, 45104380.0], 'episode_lengths': [730, 3390, 747, 540, 3801, 909, 656, 1589, 972, 2441, 1836, 849, 1334, 1120, 873, 508, 691, 425, 306, 837, 1132, 1066, 3569, 379, 890, 702, 1848, 6496, 6497, 351, 3439, 2718, 690, 3329, 853, 1576, 5724, 3685, 1360, 1666, 4722, 628, 782, 1068, 3887, 4505, 5169, 1630, 4677, 282, 6214, 3094, 5928, 1574, 4438, 1404, 612, 6314, 1183, 703, 4251, 1875, 3581, 2524, 3773, 1265, 1301, 3802, 1140, 1777, 1113, 988, 958, 2481, 1081, 2495, 1297, 1247, 1701, 2221, 1544, 3548, 1904, 3453, 1307, 1778, 1781, 3200, 822, 6040, 2842, 982, 655, 1242, 1469, 4474, 2773, 4652, 1894, 1236]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.24424259863389555, 'mean_inference_ms': 1.043864837820923, 'mean_action_processing_ms': 0.09701728577418037, 'mean_env_wait_ms': 24.024009890544747, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.004858493804931641, 'StateBufferConnector_ms': 0.0034987926483154297, 'ViewRequirementAgentConnector_ms': 0.09767889976501465}}
The agent encountered 1 episodes this iteration...
The agent encountered 3 episodes this iteration...
The agent encountered 1 episodes this iteration...
The agent encountered 1 episodes this iteration...
The agent encountered 2 episodes this iteration...
Model saved at iteration 105, steps trained: 1024.0, sample results: {'episode_reward_max': 205192380.0, 'episode_reward_min': 9095200.0, 'episode_reward_mean': 75722031.4, 'episode_len_mean': 2283.89, 'episode_media': {}, 'episodes_this_iter': 2, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [33851090.0, 68254600.0, 57919450.0, 32074670.0, 44456140.0, 36038220.0, 32016370.0, 18252040.0, 23553630.0, 14020750.0, 10926860.0, 31222990.0, 38700190.0, 34421110.0, 98690250.0, 13487820.0, 27009470.0, 21716660.0, 64452540.0, 205192380.0, 204013710.0, 11293570.0, 94577750.0, 102325970.0, 20669000.0, 126299420.0, 31175140.0, 57970730.0, 185744920.0, 134294240.0, 46307890.0, 56865050.0, 163953400.0, 22543190.0, 23567620.0, 36255420.0, 136838430.0, 155421110.0, 158968930.0, 60888430.0, 143607720.0, 9095200.0, 198512080.0, 83403960.0, 199083530.0, 54289150.0, 133445420.0, 39986550.0, 18204320.0, 202626240.0, 37419370.0, 21923690.0, 147378130.0, 69302720.0, 131046070.0, 69611440.0, 118068340.0, 47855840.0, 48032500.0, 133794120.0, 38297310.0, 63225910.0, 34523960.0, 32767060.0, 29220100.0, 92026300.0, 36345150.0, 89176030.0, 45856070.0, 46632810.0, 52776170.0, 76140630.0, 57145780.0, 123274140.0, 54110500.0, 117150860.0, 47848260.0, 55058340.0, 65158270.0, 101761500.0, 27446690.0, 190871510.0, 85419600.0, 36377520.0, 20833190.0, 43643920.0, 52105410.0, 147578760.0, 94435840.0, 157970760.0, 70628800.0, 45104380.0, 78257160.0, 120873030.0, 50189310.0, 27158830.0, 130524710.0, 86043370.0, 123950640.0, 81377020.0], 'episode_lengths': [972, 2441, 1836, 849, 1334, 1120, 873, 508, 691, 425, 306, 837, 1132, 1066, 3569, 379, 890, 702, 1848, 6496, 6497, 351, 3439, 2718, 690, 3329, 853, 1576, 5724, 3685, 1360, 1666, 4722, 628, 782, 1068, 3887, 4505, 5169, 1630, 4677, 282, 6214, 3094, 5928, 1574, 4438, 1404, 612, 6314, 1183, 703, 4251, 1875, 3581, 2524, 3773, 1265, 1301, 3802, 1140, 1777, 1113, 988, 958, 2481, 1081, 2495, 1297, 1247, 1701, 2221, 1544, 3548, 1904, 3453, 1307, 1778, 1781, 3200, 822, 6040, 2842, 982, 655, 1242, 1469, 4474, 2773, 4652, 1894, 1236, 2172, 3431, 1730, 731, 3898, 2421, 4149, 2414]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.24420172204607885, 'mean_inference_ms': 1.043962375330393, 'mean_action_processing_ms': 0.0970085984716473, 'mean_env_wait_ms': 24.025510767412133, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.004678010940551758, 'StateBufferConnector_ms': 0.003497600555419922, 'ViewRequirementAgentConnector_ms': 0.09745121002197266}}
The agent encountered 4 episodes this iteration...
The agent encountered 1 episodes this iteration...
The agent encountered 2 episodes this iteration...
The agent encountered 2 episodes this iteration...
The agent encountered 1 episodes this iteration...
slurmstepd-dh-node12: error: *** JOB 129610 ON dh-node12 CANCELLED AT 2023-11-29T11:26:35 ***
