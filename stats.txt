Model saved at iteration 0, steps trained: 64.0, sample results: {'episode_reward_max': nan, 'episode_reward_min': nan, 'episode_reward_mean': nan, 'episode_len_mean': nan, 'episode_media': {}, 'episodes_this_iter': 0, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [], 'episode_lengths': []}, 'sampler_perf': {}, 'num_faulty_episodes': 0, 'connector_metrics': {}}
Model saved at iteration 5, steps trained: 64.0, sample results: {'episode_reward_max': 32381.400000000038, 'episode_reward_min': 3108.0000000000014, 'episode_reward_mean': 18520.94000000002, 'episode_len_mean': 66.4, 'episode_media': {}, 'episodes_this_iter': 1, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [32381.400000000038, 31428.200000000033, 3108.0000000000014, 3180.1000000000017, 22507.00000000001], 'episode_lengths': [92, 92, 34, 34, 80]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.7523102709763837, 'mean_inference_ms': 2.210557160176007, 'mean_action_processing_ms': 0.1657703330105031, 'mean_env_wait_ms': 156.52012631548303, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.0, 'StateBufferConnector_ms': 0.0, 'ViewRequirementAgentConnector_ms': 0.2760457992553711}}
Model saved at iteration 10, steps trained: 64.0, sample results: {'episode_reward_max': 33950.000000000015, 'episode_reward_min': 3108.0000000000014, 'episode_reward_mean': 24143.74444444446, 'episode_len_mean': 77.0, 'episode_media': {}, 'episodes_this_iter': 1, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [32381.400000000038, 31428.200000000033, 3108.0000000000014, 3180.1000000000017, 22507.00000000001, 31806.800000000032, 33950.000000000015, 26055.899999999972, 32876.30000000003], 'episode_lengths': [92, 92, 34, 34, 80, 86, 96, 91, 88]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.7591279441811083, 'mean_inference_ms': 2.217442347368397, 'mean_action_processing_ms': 0.17676862401994445, 'mean_env_wait_ms': 157.94538321462664, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.011155340406629775, 'StateBufferConnector_ms': 0.0, 'ViewRequirementAgentConnector_ms': 0.27623441484239364}}
Model saved at iteration 15, steps trained: 64.0, sample results: {'episode_reward_max': 33950.000000000015, 'episode_reward_min': 3108.0000000000014, 'episode_reward_mean': 22183.00000000001, 'episode_len_mean': 72.85714285714286, 'episode_media': {}, 'episodes_this_iter': 1, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [32381.400000000038, 31428.200000000033, 3108.0000000000014, 3180.1000000000017, 22507.00000000001, 31806.800000000032, 33950.000000000015, 26055.899999999972, 32876.30000000003, 3180.1000000000017, 3180.1000000000017, 33346.20000000003, 31655.99999999997, 21905.90000000001], 'episode_lengths': [92, 92, 34, 34, 80, 86, 96, 91, 88, 34, 35, 91, 86, 81]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.7661260356168327, 'mean_inference_ms': 2.2203850915003307, 'mean_action_processing_ms': 0.1851577594380746, 'mean_env_wait_ms': 158.69763862207628, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.007171290261404855, 'StateBufferConnector_ms': 0.0, 'ViewRequirementAgentConnector_ms': 0.2818805830819266}}
Model saved at iteration 20, steps trained: 64.0, sample results: {'episode_reward_max': 33950.000000000015, 'episode_reward_min': 3108.0000000000014, 'episode_reward_mean': 21213.911111111123, 'episode_len_mean': 71.11111111111111, 'episode_media': {}, 'episodes_this_iter': 0, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [32381.400000000038, 31428.200000000033, 3108.0000000000014, 3180.1000000000017, 22507.00000000001, 31806.800000000032, 33950.000000000015, 26055.899999999972, 32876.30000000003, 3180.1000000000017, 3180.1000000000017, 33346.20000000003, 31655.99999999997, 21905.90000000001, 32443.70000000003, 3108.0000000000014, 3180.1000000000017, 32556.600000000046], 'episode_lengths': [92, 92, 34, 34, 80, 86, 96, 91, 88, 34, 35, 91, 86, 81, 89, 38, 34, 99]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.7674411656809413, 'mean_inference_ms': 2.220219120469085, 'mean_action_processing_ms': 0.1875276289603569, 'mean_env_wait_ms': 159.01044842397653, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.005577670203314887, 'StateBufferConnector_ms': 0.0, 'ViewRequirementAgentConnector_ms': 0.30315187242296004}}
Model saved at iteration 25, steps trained: 64.0, sample results: {'episode_reward_max': 36018.50000000005, 'episode_reward_min': 3108.0000000000014, 'episode_reward_mean': 19983.004347826096, 'episode_len_mean': 68.91304347826087, 'episode_media': {}, 'episodes_this_iter': 0, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [32381.400000000038, 31428.200000000033, 3108.0000000000014, 3180.1000000000017, 22507.00000000001, 31806.800000000032, 33950.000000000015, 26055.899999999972, 32876.30000000003, 3180.1000000000017, 3180.1000000000017, 33346.20000000003, 31655.99999999997, 21905.90000000001, 32443.70000000003, 3108.0000000000014, 3180.1000000000017, 32556.600000000046, 32224.900000000034, 3180.1000000000017, 36018.50000000005, 3180.1000000000017, 3155.1000000000017], 'episode_lengths': [92, 92, 34, 34, 80, 86, 96, 91, 88, 34, 35, 91, 86, 81, 89, 38, 34, 99, 90, 34, 105, 34, 42]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.7687690573152564, 'mean_inference_ms': 2.2202937809818106, 'mean_action_processing_ms': 0.18969678036972407, 'mean_env_wait_ms': 159.28948792223642, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.004365133202594259, 'StateBufferConnector_ms': 0.0, 'ViewRequirementAgentConnector_ms': 0.2705221590788468}}
Model saved at iteration 30, steps trained: 64.0, sample results: {'episode_reward_max': 38542.700000000026, 'episode_reward_min': 3108.0000000000014, 'episode_reward_mean': 22087.13333333335, 'episode_len_mean': 72.81481481481481, 'episode_media': {}, 'episodes_this_iter': 1, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [32381.400000000038, 31428.200000000033, 3108.0000000000014, 3180.1000000000017, 22507.00000000001, 31806.800000000032, 33950.000000000015, 26055.899999999972, 32876.30000000003, 3180.1000000000017, 3180.1000000000017, 33346.20000000003, 31655.99999999997, 21905.90000000001, 32443.70000000003, 3108.0000000000014, 3180.1000000000017, 32556.600000000046, 32224.900000000034, 3180.1000000000017, 36018.50000000005, 3180.1000000000017, 3155.1000000000017, 32475.100000000042, 32100.000000000036, 38542.700000000026, 33625.70000000003], 'episode_lengths': [92, 92, 34, 34, 80, 86, 96, 91, 88, 34, 35, 91, 86, 81, 89, 38, 34, 99, 90, 34, 105, 34, 42, 87, 91, 107, 96]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.7688447243379689, 'mean_inference_ms': 2.2219009375716086, 'mean_action_processing_ms': 0.1907948921021232, 'mean_env_wait_ms': 159.57815157049293, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.0037184468022099246, 'StateBufferConnector_ms': 0.0, 'ViewRequirementAgentConnector_ms': 0.2623540383798105}}
Model saved at iteration 35, steps trained: 64.0, sample results: {'episode_reward_max': 38542.700000000026, 'episode_reward_min': 3108.0000000000014, 'episode_reward_mean': 22152.370967741954, 'episode_len_mean': 72.83870967741936, 'episode_media': {}, 'episodes_this_iter': 1, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [32381.400000000038, 31428.200000000033, 3108.0000000000014, 3180.1000000000017, 22507.00000000001, 31806.800000000032, 33950.000000000015, 26055.899999999972, 32876.30000000003, 3180.1000000000017, 3180.1000000000017, 33346.20000000003, 31655.99999999997, 21905.90000000001, 32443.70000000003, 3108.0000000000014, 3180.1000000000017, 32556.600000000046, 32224.900000000034, 3180.1000000000017, 36018.50000000005, 3180.1000000000017, 3155.1000000000017, 32475.100000000042, 32100.000000000036, 38542.700000000026, 33625.70000000003, 22994.900000000016, 3108.0000000000014, 31797.700000000015, 32470.30000000002], 'episode_lengths': [92, 92, 34, 34, 80, 86, 96, 91, 88, 34, 35, 91, 86, 81, 89, 38, 34, 99, 90, 34, 105, 34, 42, 87, 91, 107, 96, 78, 36, 88, 90]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.7702168506807733, 'mean_inference_ms': 2.2254666786247292, 'mean_action_processing_ms': 0.19143350871848214, 'mean_env_wait_ms': 159.95019258154662, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.0035309022472750757, 'StateBufferConnector_ms': 0.0032217271866336944, 'ViewRequirementAgentConnector_ms': 0.25363968264672065}}
Model saved at iteration 40, steps trained: 64.0, sample results: {'episode_reward_max': 38542.700000000026, 'episode_reward_min': 3108.0000000000014, 'episode_reward_mean': 22523.38857142859, 'episode_len_mean': 73.31428571428572, 'episode_media': {}, 'episodes_this_iter': 1, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [32381.400000000038, 31428.200000000033, 3108.0000000000014, 3180.1000000000017, 22507.00000000001, 31806.800000000032, 33950.000000000015, 26055.899999999972, 32876.30000000003, 3180.1000000000017, 3180.1000000000017, 33346.20000000003, 31655.99999999997, 21905.90000000001, 32443.70000000003, 3108.0000000000014, 3180.1000000000017, 32556.600000000046, 32224.900000000034, 3180.1000000000017, 36018.50000000005, 3180.1000000000017, 3155.1000000000017, 32475.100000000042, 32100.000000000036, 38542.700000000026, 33625.70000000003, 22994.900000000016, 3108.0000000000014, 31797.700000000015, 32470.30000000002, 32232.10000000003, 3118.5000000000014, 33490.20000000003, 32754.300000000036], 'episode_lengths': [92, 92, 34, 34, 80, 86, 96, 91, 88, 34, 35, 91, 86, 81, 89, 38, 34, 99, 90, 34, 105, 34, 42, 87, 91, 107, 96, 78, 36, 88, 90, 89, 39, 91, 89]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.7712459910215969, 'mean_inference_ms': 2.2285425870195774, 'mean_action_processing_ms': 0.19226999542828055, 'mean_env_wait_ms': 160.2663510656653, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.0031273705618722098, 'StateBufferConnector_ms': 0.0028535297938755582, 'ViewRequirementAgentConnector_ms': 0.26477132524762836}}
Model saved at iteration 45, steps trained: 64.0, sample results: {'episode_reward_max': 38542.700000000026, 'episode_reward_min': 3108.0000000000014, 'episode_reward_mean': 23414.37692307694, 'episode_len_mean': 75.2051282051282, 'episode_media': {}, 'episodes_this_iter': 1, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [32381.400000000038, 31428.200000000033, 3108.0000000000014, 3180.1000000000017, 22507.00000000001, 31806.800000000032, 33950.000000000015, 26055.899999999972, 32876.30000000003, 3180.1000000000017, 3180.1000000000017, 33346.20000000003, 31655.99999999997, 21905.90000000001, 32443.70000000003, 3108.0000000000014, 3180.1000000000017, 32556.600000000046, 32224.900000000034, 3180.1000000000017, 36018.50000000005, 3180.1000000000017, 3155.1000000000017, 32475.100000000042, 32100.000000000036, 38542.700000000026, 33625.70000000003, 22994.900000000016, 3108.0000000000014, 31797.700000000015, 32470.30000000002, 32232.10000000003, 3118.5000000000014, 33490.20000000003, 32754.300000000036, 22302.000000000007, 33561.40000000004, 36990.60000000001, 31988.100000000035], 'episode_lengths': [92, 92, 34, 34, 80, 86, 96, 91, 88, 34, 35, 91, 86, 81, 89, 38, 34, 99, 90, 34, 105, 34, 42, 87, 91, 107, 96, 78, 36, 88, 90, 89, 39, 91, 89, 89, 91, 99, 88]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.7716071415858231, 'mean_inference_ms': 2.23142569100623, 'mean_action_processing_ms': 0.19357710003058626, 'mean_env_wait_ms': 160.52483763577072, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.0053735879751352165, 'StateBufferConnector_ms': 0.0025608600714267828, 'ViewRequirementAgentConnector_ms': 0.26001563439002406}}
Model saved at iteration 50, steps trained: 64.0, sample results: {'episode_reward_max': 38542.700000000026, 'episode_reward_min': 3108.0000000000014, 'episode_reward_mean': 24113.0523809524, 'episode_len_mean': 76.83333333333333, 'episode_media': {}, 'episodes_this_iter': 1, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [32381.400000000038, 31428.200000000033, 3108.0000000000014, 3180.1000000000017, 22507.00000000001, 31806.800000000032, 33950.000000000015, 26055.899999999972, 32876.30000000003, 3180.1000000000017, 3180.1000000000017, 33346.20000000003, 31655.99999999997, 21905.90000000001, 32443.70000000003, 3108.0000000000014, 3180.1000000000017, 32556.600000000046, 32224.900000000034, 3180.1000000000017, 36018.50000000005, 3180.1000000000017, 3155.1000000000017, 32475.100000000042, 32100.000000000036, 38542.700000000026, 33625.70000000003, 22994.900000000016, 3108.0000000000014, 31797.700000000015, 32470.30000000002, 32232.10000000003, 3118.5000000000014, 33490.20000000003, 32754.300000000036, 22302.000000000007, 33561.40000000004, 36990.60000000001, 31988.100000000035, 30733.100000000035, 34342.90000000004, 34511.50000000004], 'episode_lengths': [92, 92, 34, 34, 80, 86, 96, 91, 88, 34, 35, 91, 86, 81, 89, 38, 34, 99, 90, 34, 105, 34, 42, 87, 91, 107, 96, 78, 36, 88, 90, 89, 39, 91, 89, 89, 91, 99, 88, 97, 96, 101]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.771346265496072, 'mean_inference_ms': 2.2333493675141227, 'mean_action_processing_ms': 0.19446739173212277, 'mean_env_wait_ms': 160.69296506870933, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.004989760262625558, 'StateBufferConnector_ms': 0.002377941494896298, 'ViewRequirementAgentConnector_ms': 0.2570407731192453}}
Model saved at iteration 0, steps trained: 64.0, sample results: {'episode_reward_max': 3194.2000000000016, 'episode_reward_min': 3194.2000000000016, 'episode_reward_mean': 3194.2000000000016, 'episode_len_mean': 37.0, 'episode_media': {}, 'episodes_this_iter': 1, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [3194.2000000000016], 'episode_lengths': [37]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 1.0441156534048228, 'mean_inference_ms': 2.42309937110314, 'mean_action_processing_ms': 0.18065525935246396, 'mean_env_wait_ms': 182.16250126178448, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.09191036224365234, 'StateBufferConnector_ms': 0.0, 'ViewRequirementAgentConnector_ms': 0.3993511199951172}}
Model saved at iteration 5, steps trained: 64.0, sample results: {'episode_reward_max': 36119.600000000006, 'episode_reward_min': 3108.0000000000014, 'episode_reward_mean': 12089.314285714292, 'episode_len_mean': 51.57142857142857, 'episode_media': {}, 'episodes_this_iter': 2, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [3194.2000000000016, 3122.1000000000013, 32793.20000000003, 3108.0000000000014, 36119.600000000006, 3108.0000000000014, 3180.1000000000017], 'episode_lengths': [37, 39, 88, 34, 95, 34, 34]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.9146573029603502, 'mean_inference_ms': 2.368125100737842, 'mean_action_processing_ms': 0.23036514853105888, 'mean_env_wait_ms': 175.1850909314839, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.013130051749093192, 'StateBufferConnector_ms': 0.0162090573992048, 'ViewRequirementAgentConnector_ms': 0.2670833042689732}}
Model saved at iteration 0, steps trained: 64.0, sample results: {'episode_reward_max': nan, 'episode_reward_min': nan, 'episode_reward_mean': nan, 'episode_len_mean': nan, 'episode_media': {}, 'episodes_this_iter': 0, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [], 'episode_lengths': []}, 'sampler_perf': {}, 'num_faulty_episodes': 0, 'connector_metrics': {}}
Model saved at iteration 0, steps trained: 64.0, sample results: {'episode_reward_max': nan, 'episode_reward_min': nan, 'episode_reward_mean': nan, 'episode_len_mean': nan, 'episode_media': {}, 'episodes_this_iter': 0, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [], 'episode_lengths': []}, 'sampler_perf': {}, 'num_faulty_episodes': 0, 'connector_metrics': {}}
Model saved at iteration 5, steps trained: 64.0, sample results: {'episode_reward_max': 30968.500000000022, 'episode_reward_min': 3180.1000000000017, 'episode_reward_mean': 15888.98000000001, 'episode_len_mean': 59.4, 'episode_media': {}, 'episodes_this_iter': 0, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [20913.700000000008, 21202.50000000001, 3180.1000000000017, 30968.500000000022, 3180.1000000000017], 'episode_lengths': [69, 76, 34, 84, 34]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.8271527729354702, 'mean_inference_ms': 2.3054216092610704, 'mean_action_processing_ms': 0.215694387918882, 'mean_env_wait_ms': 163.74872538139613, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.04127979278564453, 'StateBufferConnector_ms': 0.0016546249389648438, 'ViewRequirementAgentConnector_ms': 0.27220726013183594}}
Model saved at iteration 10, steps trained: 64.0, sample results: {'episode_reward_max': 32995.20000000003, 'episode_reward_min': 3108.0000000000014, 'episode_reward_mean': 15283.472727272734, 'episode_len_mean': 58.90909090909091, 'episode_media': {}, 'episodes_this_iter': 1, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [20913.700000000008, 21202.50000000001, 3180.1000000000017, 30968.500000000022, 3180.1000000000017, 32995.20000000003, 3108.0000000000014, 22723.9, 23630.199999999993, 3108.0000000000014, 3108.0000000000014], 'episode_lengths': [69, 76, 34, 84, 34, 92, 34, 77, 80, 34, 34]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.8506759802337012, 'mean_inference_ms': 2.335086209077774, 'mean_action_processing_ms': 0.2125557722180379, 'mean_env_wait_ms': 165.060144756677, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.01876354217529297, 'StateBufferConnector_ms': 0.009881366382945667, 'ViewRequirementAgentConnector_ms': 0.27016726407137787}}
Model saved at iteration 15, steps trained: 64.0, sample results: {'episode_reward_max': 34098.50000000004, 'episode_reward_min': 3108.0000000000014, 'episode_reward_mean': 14601.65882352942, 'episode_len_mean': 57.0, 'episode_media': {}, 'episodes_this_iter': 1, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [20913.700000000008, 21202.50000000001, 3180.1000000000017, 30968.500000000022, 3180.1000000000017, 32995.20000000003, 3108.0000000000014, 22723.9, 23630.199999999993, 3108.0000000000014, 3108.0000000000014, 34098.50000000004, 33435.30000000004, 3180.1000000000017, 3108.0000000000014, 3108.0000000000014, 3180.1000000000017], 'episode_lengths': [69, 76, 34, 84, 34, 92, 34, 77, 80, 34, 34, 94, 91, 34, 34, 34, 34]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.8470500012646419, 'mean_inference_ms': 2.3371122075003843, 'mean_action_processing_ms': 0.21320947015605152, 'mean_env_wait_ms': 164.83666816528373, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.018037066740148208, 'StateBufferConnector_ms': 0.006393825306611902, 'ViewRequirementAgentConnector_ms': 0.2548077527214499}}
Model saved at iteration 20, steps trained: 64.0, sample results: {'episode_reward_max': 37218.70000000005, 'episode_reward_min': 3108.0000000000014, 'episode_reward_mean': 15832.35714285715, 'episode_len_mean': 59.904761904761905, 'episode_media': {}, 'episodes_this_iter': 0, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [20913.700000000008, 21202.50000000001, 3180.1000000000017, 30968.500000000022, 3180.1000000000017, 32995.20000000003, 3108.0000000000014, 22723.9, 23630.199999999993, 3108.0000000000014, 3108.0000000000014, 34098.50000000004, 33435.30000000004, 3180.1000000000017, 3108.0000000000014, 3108.0000000000014, 3180.1000000000017, 37218.70000000005, 21405.80000000001, 22518.8, 3108.0000000000014], 'episode_lengths': [69, 76, 34, 84, 34, 92, 34, 77, 80, 34, 34, 94, 91, 34, 34, 34, 34, 108, 72, 75, 34]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.8430547800461671, 'mean_inference_ms': 2.3338959691520618, 'mean_action_processing_ms': 0.2119627867813486, 'mean_env_wait_ms': 164.53103396315515, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.02135435740152995, 'StateBufferConnector_ms': 0.005175953819638207, 'ViewRequirementAgentConnector_ms': 0.25573571523030597}}
